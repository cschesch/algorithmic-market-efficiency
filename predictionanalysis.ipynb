{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIwTcxqN78ne"
   },
   "source": [
    "# Algorithmic Market Efficiency - Prediction Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1581602365118,
     "user": {
      "displayName": "Constantin Schesch",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDzShDFJXhe-KA6VubV_vLoKhEeq2HWJUHqYaWV9A=s64",
      "userId": "12583853904578710112"
     },
     "user_tz": -60
    },
    "id": "Ga31qcQ478nh",
    "outputId": "6e0f5391-7a7c-4dd6-ab4d-8f4a713a9f29"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as mtick\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy.stats.mstats import gmean\n",
    "     \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import pylatex\n",
    "from pylatex import Document, Section, Subsection, Tabular, Math, TikZ, Axis\n",
    "from pylatex import Plot, Figure, Matrix, Alignat, NoEscape, MultiColumn, MultiRow\n",
    "from pylatex.utils import italic, NoEscape\n",
    "pylatex.config.active = pylatex.config.Version2\n",
    "\n",
    "print(\"current directory is : \" + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MApLvHeqFJ4A"
   },
   "outputs": [],
   "source": [
    "filepath = 'output/'\n",
    "if not os.path.isdir(filepath): os.mkdir(filepath)\n",
    "\n",
    "graphpath = 'graphs/'\n",
    "if not os.path.isdir(graphpath): os.mkdir(graphpath)\n",
    "    \n",
    "tablespath = 'tables/'\n",
    "if not os.path.isdir(tablespath): os.mkdir(tablespath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Doa2sI7L78nk"
   },
   "source": [
    "## Data code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHidnp2s78nl"
   },
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg2aGmhfrFfs"
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open('data/df_mktvars', 'rb'))\n",
    "\n",
    "dates, dtdates, permnos, vrs, facs, returns, T, N, V, F =  pickle.load(open('data/info', 'rb'))\n",
    "\n",
    "rf, mkt_rf, smb, hml, rmw, cma, mom = pickle.load(open('data/misc', 'rb'))\n",
    "\n",
    "predictions = pickle.load(open('mloutput/predictions', 'rb'))\n",
    "scores = pickle.load(open('mloutput/scores', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mktcaps = df[['date','permno','mktcap',]].pivot(index='date', columns='permno', values='mktcap')\n",
    "\n",
    "mktcapshares = mktcaps.to_numpy().T / np.nansum(mktcaps.to_numpy(), axis=1)\n",
    "mktcapshares = mktcapshares.T\n",
    "mktcapshares = pd.DataFrame(mktcapshares, index=mktcaps.index,columns=mktcaps.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 111140,
     "status": "ok",
     "timestamp": 1581602475161,
     "user": {
      "displayName": "Constantin Schesch",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDzShDFJXhe-KA6VubV_vLoKhEeq2HWJUHqYaWV9A=s64",
      "userId": "12583853904578710112"
     },
     "user_tz": -60
    },
    "id": "eCfvs2eVS48q",
    "outputId": "03c18a27-7acf-49eb-ce81-71915b213b9b"
   },
   "outputs": [],
   "source": [
    "methods = ['ols', 'lasso', 'ridge', 'enet', 'pcr', 'pls','tree', 'forest', 'gbrt', 'nn1', 'nn2', 'nn3', 'nn5', 'nn10']\n",
    "M = len(methods)\n",
    "methodlabels = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS','Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib utils:\n",
    "\n",
    "def basefig(a, b):\n",
    "    fig = plt.figure(figsize=(a,b))\n",
    "    ax = fig.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.margins(y=0, x=0)\n",
    "    return fig, ax\n",
    "\n",
    "def labeledline(values, label, offset, ax, width=None, color=None, style=None):\n",
    "    \n",
    "    ax.plot(dtdates, values, linewidth=width, color=color, linestyle=style)\n",
    "    ax.annotate(s=label, xy=(dtdates[-1],values[-1]), xytext=(5,offset), textcoords='offset points', va='center')\n",
    "\n",
    "def labeledlines(valueslist, labellist, offsetlist, ax):\n",
    "    \n",
    "    for i, label in enumerate(labellist):\n",
    "                \n",
    "        values = valueslist[methods[i]]\n",
    "        offset = offsetlist[i]\n",
    "\n",
    "        labeledline(values, label, offset, ax)\n",
    "        \n",
    "plt.rcParams['font.sans-serif']=['Fira Sans']\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 'medium'\n",
    "plt.rcParams['axes.titlepad'] = 25\n",
    "plt.rcParams['axes.labelpad'] = 10\n",
    "\n",
    "plt.rcParams['xtick.major.pad'] = 4\n",
    "plt.rcParams['ytick.major.pad'] = 5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.fancybox'] = False\n",
    "plt.rcParams['legend.framealpha'] = 1\n",
    "plt.rcParams['legend.edgecolor'] = 'black'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"tab20\")\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "after1980 = [datetime.date(1980, 1, 1), datetime.date(2016, 12, 1)]\n",
    "from80to15 = [datetime.date(1980, 1, 1), datetime.date(2015, 12, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeledline(values, label, offset, ax, width=None, color=None, style=None, alpha=None):\n",
    "        \n",
    "    ax.plot(dtdates, values, linewidth=width, color=color, linestyle=style, alpha=alpha)\n",
    "    ax.annotate(s=label, xy=(dtdates[-1],values[-1]), xytext=(5,offset), textcoords='offset points', va='center')\n",
    "\n",
    "def labeledlines(valueslist, labellist, offsetlist, ax, alpha=None):\n",
    "            \n",
    "    if len(labellist) == len(methodlabels) :\n",
    "        \n",
    "        for i, label in enumerate(labellist):\n",
    "\n",
    "            values = valueslist[methods[i]]\n",
    "            offset = offsetlist[i]\n",
    "\n",
    "            labeledline(values, label, offset, ax, width=None, color=methodcolors[i], alpha=alpha)\n",
    "    \n",
    "    elif len(labellist) == len(fewmethodlabels) :\n",
    "            \n",
    "        for i, label in enumerate(labellist):\n",
    "            \n",
    "            values = valueslist[fewmethods[i]]\n",
    "            offset = offsetlist[i]\n",
    "            \n",
    "            labeledline(values, label, offset, ax, width=None, color=fewmethodcolors[i], alpha=alpha)\n",
    "    else:\n",
    "        print(\"Error: labellist length not reconised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "\n",
    "fewmethods_idx = [True, False, False, False, False, False, True, True, False, True, True, True, False, False]\n",
    "\n",
    "fewmethods = list(itertools.compress(methods, fewmethods_idx)) \n",
    "\n",
    "fewmethodlabels = list(itertools.compress(methodlabels, fewmethods_idx)) \n",
    "\n",
    "colorcycle = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "methodcolors = colorcycle[:len(methodlabels)]\n",
    "#fewmethodcolors = list(itertools.compress(methodcolors, fewmethods_idx)) \n",
    "fewmethodcolors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:pink']\n",
    "\n",
    "Mfew = len(fewmethods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma(sample, window_size=12*3, centered=False):\n",
    "    roll = pd.Series(sample).rolling(window=window_size, center=centered)\n",
    "    ma = roll.mean().to_numpy()\n",
    "    return ma\n",
    "\n",
    "def movingaverage(sample, window_size=12*5, centered=True):\n",
    "    roll = pd.Series(sample).rolling(window=window_size, center=centered)\n",
    "    ma = roll.mean().to_numpy()\n",
    "    return ma\n",
    "    \n",
    "def dictma(sampledict, window_size=12*3, centered=False):\n",
    "    madict = sampledict.copy()\n",
    "    for key in sampledict:\n",
    "        madict[key] = ma(sampledict[key], window_size, centered)\n",
    "    return madict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_pctile = 100 * returns.rank(pct=True, axis=1)\n",
    "mktcaps_pctile = 100 * mktcaps.rank(pct=True, axis=1)\n",
    "\n",
    "predictions_pctile = dict()\n",
    "for method in methods:\n",
    "    predictions_pctile[method] = 100 * predictions[method].rank(pct=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Section Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_years = (dates / 10000).astype('int')\n",
    "years = np.unique(dates_years)\n",
    "\n",
    "def intopercentile(x):\n",
    "    return 100 * sp.stats.rankdata(x) / (np.max(sp.stats.rankdata(x[~np.isnan(x)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Tabular('lccccc')\n",
    "\n",
    "table.add_row(('', MultiColumn(2, align='c', data='In-sample (2006-2015)'), '', MultiColumn(2, align='c', data='Out-of-sample (2016)')))\n",
    "\n",
    "table.add_hline(2, 3)\n",
    "table.add_hline(5, 6)\n",
    "\n",
    "table.add_row(('', '   R2   ', '   rho   ', '', 'R2', 'rho'))\n",
    "\n",
    "for (i,method) in enumerate(methods):    \n",
    "\n",
    "    r2_test = scores.loc[(20160129,method,'test','R2')]\n",
    "    spearman_test = scores.loc[(20160129,method,'test','spearman')]\n",
    "\n",
    "    r2_oos = scores.loc[(20160129,method,'oos','R2')]\n",
    "    spearman_oos = scores.loc[(20160129,method,'oos','spearman')]\n",
    "\n",
    "    table.add_row((methodlabels[i],\n",
    "                    '{:.2f}'.format(r2_test), '{:.2f}'.format(spearman_test), '',\n",
    "                    '{:.2f}'.format(r2_oos), '{:.2f}'.format(spearman_oos)))\n",
    "\n",
    "\n",
    "table.generate_tex('tables/scores_2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method='nn3'\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18,6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "date = dates[-1]\n",
    "\n",
    "prediction = predictions[method].loc[date].to_numpy().flatten()\n",
    "currets = returns.loc[date].to_numpy().flatten()\n",
    "\n",
    "prediction = prediction[~np.isnan(prediction)]\n",
    "currets = currets[~np.isnan(currets)]\n",
    "\n",
    "alphaparam = 0.05\n",
    "\n",
    "print(prediction.shape)\n",
    "print(currets.shape)\n",
    "\n",
    "\n",
    "ax[0].plot(currets, prediction, 'ko', alpha=alphaparam)\n",
    "ax[0].plot(np.sort(currets), np.zeros_like(prediction), 'k--')\n",
    "ax[0].axvline(ymin=min(prediction), ymax=max(prediction), color='k', linestyle='--')\n",
    "ax[0].set_xlabel('Actual Monthly Return')\n",
    "ax[0].set_ylabel('Predicted Monthly Return')\n",
    "ax[0].set_ylim([-0.25, 0.5])\n",
    "ax[0].set_xlim([-0.25, 0.5])\n",
    "\n",
    "\n",
    "ax[1].plot(intopercentile(currets), prediction, 'ko', alpha=alphaparam)\n",
    "ax[1].plot(np.sort(intopercentile(currets)), np.zeros_like(prediction), 'k--')\n",
    "ax[1].set_xlabel('Actual Monthly Return, Percentile')\n",
    "ax[1].set_ylabel('Predicted Monthly Return')\n",
    "ax[1].set_ylim([-0.25, 0.5])\n",
    "\n",
    "ax[2].plot(intopercentile(currets), intopercentile(prediction), 'ko', alpha=alphaparam)\n",
    "ax[2].set_xlabel('Actual Monthly Return, Percentile')\n",
    "ax[2].set_ylabel('Predicted Monthly Return, Percentile')\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.suptitle(\"Predicted and Actual Monthly Returns - 3-Layer Neural Network - December 2016\", size=18, y=1.05)\n",
    "\n",
    "plt.savefig(graphpath+'predictionsvsreturns_explanationplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdates = [19800530, 19830729, 20001130, 20040528, 20151030, 20161230]\n",
    "\n",
    "for date in graphdates:\n",
    "\n",
    "    def predictionsvsreturnsplot(method, date, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "        prediction = predictions[method].loc[date].to_numpy().flatten()\n",
    "        currets = returns.loc[date].to_numpy().flatten()\n",
    "\n",
    "        prediction = prediction[~np.isnan(prediction)]\n",
    "        currets = currets[~np.isnan(currets)]\n",
    "\n",
    "        binedges = np.linspace(np.min(intopercentile(prediction)), np.max(intopercentile(prediction)), 21)\n",
    "        binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(intopercentile(currets), prediction, 'mean', bins=binedges)\n",
    "\n",
    "        R2 = r2_score(currets, prediction)\n",
    "        spearman = sp.stats.spearmanr(currets, prediction)[0]\n",
    "\n",
    "        ax.plot(np.sort(intopercentile(currets)), np.sort(currets), 'C2--', label=\"Actual monthly returns\")\n",
    "        ax.plot(intopercentile(currets), prediction, 'ko', alpha=alphaparam)\n",
    "        ax.plot(intopercentile(currets), np.full_like(intopercentile(currets), np.nanmean(prediction)), 'k-', label='Predicted returns: Overall average')\n",
    "        ax.plot(binpts, means, 'C0o', label='Predicted returns: Vigintile avg.')\n",
    "        ax.set_xlabel('Actual Monthly Return, Percentile')\n",
    "        ax.set_ylabel('Predicted Monthly Return')\n",
    "        ax.set_ylim([-0.5, 0.5])\n",
    "        ax.set_title(title + ' - R2: ' + '{:.2f}'.format(R2) + ' - Spearman Rho: ' + '{:.2f}'.format(spearman))\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18,12))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    predictionsvsreturnsplot(\"ols\", date, ax[0], alphaparam=0.02, title=\"Ordinary Least Squares\")\n",
    "\n",
    "    predictionsvsreturnsplot(\"pcr\", date, ax[1], alphaparam=0.02, title=\"Principal Components Regression\")\n",
    "\n",
    "    predictionsvsreturnsplot(\"forest\", date, ax[2], alphaparam=0.02, title=\"Random Forest\")\n",
    "\n",
    "    predictionsvsreturnsplot(\"nn3\", date, ax[3], alphaparam=0.02, title=\"3-Layer Neural Network\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(\"Predicted and Actual Monthly Returns - Cross-Section Analysis - \" + str(date), size=18, y=1.02)\n",
    "\n",
    "    plt.savefig(graphpath+'predictionsvsreturnsplot_'+str(date)+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdates = [19800530, 19830729, 20001130, 20040528, 20151030, 20161230]\n",
    "\n",
    "for date in graphdates:\n",
    "\n",
    "    def returnsvspredictionsplot(method, date, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "        prediction = predictions[method].loc[date].to_numpy().flatten()\n",
    "        currets = returns.loc[date].to_numpy().flatten()\n",
    "\n",
    "        prediction = prediction[~np.isnan(prediction)]\n",
    "        currets = currets[~np.isnan(currets)]\n",
    "\n",
    "        binedges = np.linspace(np.min(intopercentile(currets)), np.max(intopercentile(currets)), 21)\n",
    "        binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(intopercentile(prediction), currets, 'mean', bins=binedges)\n",
    "        stds, _, _ = sp.stats.binned_statistic(intopercentile(prediction), currets, 'std', bins=binedges)\n",
    "\n",
    "        R2 = r2_score(currets, prediction)\n",
    "        spearman = sp.stats.spearmanr(currets, prediction)[0]\n",
    "\n",
    "        ax.plot(np.sort(intopercentile(prediction)), np.sort(prediction), 'C0--', label=\"Predicted monthly returns\")\n",
    "        ax.plot(intopercentile(prediction), currets, 'ko', alpha=alphaparam)\n",
    "        ax.plot(intopercentile(prediction), np.full_like(intopercentile(prediction), np.nanmean(currets)), 'k-', label='Actual returns: Overall average')\n",
    "        ax.plot(binpts, means, 'C2o', label='Actual returns: Vigintile avg. (std. dev.)')\n",
    "        ax.set_xlabel('Predicted Monthly Return, Percentile')\n",
    "        ax.set_ylabel('Actual Monthly Return')\n",
    "        ax.set_ylim([-0.3, 0.3])\n",
    "        ax.set_title(title + ' - R2: ' + '{:.2f}'.format(R2) + ' - Spearman Rho: ' + '{:.2f}'.format(spearman))\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18,12))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    returnsvspredictionsplot(\"ols\", date, ax[0], alphaparam=0.02, title=\"Ordinary Least Squares\")\n",
    "\n",
    "    returnsvspredictionsplot(\"pcr\", date, ax[1], alphaparam=0.02, title=\"Principal Components Regression\")\n",
    "\n",
    "    returnsvspredictionsplot(\"forest\", date, ax[2], alphaparam=0.02, title=\"Random Forest\")\n",
    "\n",
    "    returnsvspredictionsplot(\"nn3\", date, ax[3], alphaparam=0.02, title=\"3-Layer Neural Network\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(\"Predicted and Actual Monthly Returns - Cross-Section Analysis - \" + str(date), size=18, y=1.02)\n",
    "\n",
    "    plt.savefig(graphpath+'returnsvspredictionsplot_'+str(date)+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorestimeseries(method, titlelabel):\n",
    "        \n",
    "    testR2_list = scores.loc[(dates,method,'test','R2')].values\n",
    "    testR2_list = pd.DataFrame(testR2_list).fillna(method='ffill').to_numpy().flatten()\n",
    "    \n",
    "    testSpearman_list = scores.loc[(dates,method,'test','spearman')].values\n",
    "    testSpearman_list = pd.DataFrame(testSpearman_list).fillna(method='ffill').to_numpy().flatten()\n",
    "    \n",
    "    oosR2_list = scores.loc[(dates,method,'oos','R2')].values\n",
    "    oosR2_list = pd.DataFrame(oosR2_list).fillna(method='ffill').to_numpy().flatten()\n",
    "    \n",
    "    oosSpearman_list = scores.loc[(dates,method,'oos','spearman')].values\n",
    "    oosSpearman_list = pd.DataFrame(oosSpearman_list).fillna(method='ffill').to_numpy().flatten()\n",
    "    \n",
    "        \n",
    "    fig, ax = plt.subplots(1, figsize=(15,4))\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    ax.plot(dtdates, testR2_list, 'C0--', alpha=0.2)\n",
    "    ax.plot(dtdates, ma(testR2_list, 12*3, centered=True), 'C0--', label='Test R2 (3-year mov. avg.)')\n",
    "    ax.plot(dtdates, oosR2_list, 'C0', alpha=0.2)\n",
    "    ax.plot(dtdates, ma(oosR2_list, 12*3, centered=True), 'C0', label='OOS R2 (3-year mov. avg.)')\n",
    "    \n",
    "    ax.plot(dtdates, testSpearman_list, 'C2--', alpha=0.2)\n",
    "    ax.plot(dtdates, ma(testSpearman_list, 12*3, centered=True), 'C2--', label='Test Spearman Rho (3-year mov. avg.)')\n",
    "    ax.plot(dtdates, oosSpearman_list, 'C2', alpha=0.2)\n",
    "    ax.plot(dtdates, ma(oosSpearman_list, 12*3, centered=True), 'C2', label='OOS Spearman Rho (3-year mov. avg.)')\n",
    "    \n",
    "    ax.plot(dtdates, np.zeros(len(dates)), 'k--', linewidth=2)\n",
    "    ax.plot(dtdates, np.ones(len(dates)), 'k--', linewidth=2)\n",
    "    ax.plot(dtdates, -0.25*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "    ax.plot(dtdates, -0.5*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "    ax.plot(dtdates, -0.75*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "    ax.plot(dtdates, 0.25*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "    ax.plot(dtdates, 0.5*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "    ax.plot(dtdates, 0.75*np.ones(len(dates)), 'k--', alpha=0.25, linewidth=1)\n",
    "\n",
    "    plt.legend(frameon=False)\n",
    "    ax.set_ylim([-1.1, 1.1])\n",
    "    ax.set_title(titlelabel, size=16, y=0.95)\n",
    "    plt.savefig(graphpath+'scorestimeseries_'+method+'.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "scorestimeseries('ols', \"Ordinary Least Squares\")\n",
    "scorestimeseries('pcr', \"Principal Components Regression\")\n",
    "scorestimeseries('forest', \"Random Forest\")\n",
    "scorestimeseries('nn3', \"3-Layer Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Portfolio Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "188VgVO178of"
   },
   "source": [
    "## Strategy computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2Y9tAhSrS48"
   },
   "outputs": [],
   "source": [
    "def longstrategy(method, percentile, valueweighted=True):\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "          \n",
    "    prediction = predictions[method].copy()\n",
    "\n",
    "    p_top = np.nanpercentile(prediction, 100 - percentile, axis=1, keepdims=True)\n",
    "\n",
    "    strategy[prediction>=p_top] = 1\n",
    "    \n",
    "    if valueweighted : strategy = strategy * mktcaps.values\n",
    "    \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True)\n",
    "    strategy = strategy / rescaler\n",
    "    \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OePekbBE78of"
   },
   "outputs": [],
   "source": [
    "def longshortstrategy(method, percentile, valueweighted=True):\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy().to_numpy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "              \n",
    "    prediction = predictions[method].copy().to_numpy()\n",
    "\n",
    "    p_top = np.nanpercentile(prediction, 100 - percentile, axis=1, keepdims=True)\n",
    "    p_bottom = np.nanpercentile(prediction, percentile, axis=1, keepdims=True)\n",
    "\n",
    "    strategy[prediction>p_top] = 1\n",
    "    strategy[prediction<p_bottom] = -1\n",
    "    \n",
    "    if valueweighted : strategy = strategy * mktcaps.values\n",
    "            \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True) / 2\n",
    "    strategy = strategy / rescaler\n",
    "    \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K--DQAUiFJ4V"
   },
   "outputs": [],
   "source": [
    "def rankstrategy(method, valueweighted=True):\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy().to_numpy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "              \n",
    "    prediction = predictions[method].copy().to_numpy()\n",
    "    prediction[np.isnan(prediction)] = np.inf\n",
    "        \n",
    "    prediction_order = np.argsort(prediction, axis=1)\n",
    "    prediction_rank = np.argsort(prediction_order, axis=1) + 1\n",
    "    \n",
    "    numberofstocks = np.count_nonzero(~np.isnan(strategy), axis=1)\n",
    "        \n",
    "    strategy = (prediction_rank.T - ((numberofstocks+1)/2)) \n",
    "    \n",
    "    strategy =  strategy.T\n",
    "    strategy[np.isnan(returns)] = np.nan\n",
    "        \n",
    "    if valueweighted : strategy = strategy * mktcaps.values\n",
    "            \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True) / 2\n",
    "    strategy = strategy / rescaler\n",
    "        \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0l7hYeMKxpN"
   },
   "source": [
    "## Return, Sharpe ratio and alpha computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjTcTryc78oi"
   },
   "outputs": [],
   "source": [
    "def strategyreturns(strategy):\n",
    "\n",
    "    if type(strategy) is not np.ndarray:\n",
    "        strategy = np.array(strategy)\n",
    "     \n",
    "    indrets = np.multiply(strategy,returns)\n",
    "\n",
    "    yearrets = np.nansum(indrets, axis=1)\n",
    "    \n",
    "    excessrets = yearrets - np.nanmean(returns,axis=1)\n",
    "\n",
    "    compoundrets = np.cumproduct(yearrets+1)\n",
    "            \n",
    "    totalret = compoundrets[-1] - 1\n",
    "\n",
    "    return yearrets, compoundrets, excessrets, totalret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS6_AzcLFJ4x"
   },
   "outputs": [],
   "source": [
    "def sharperatios(yearrets, n_periods):\n",
    "               \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "        \n",
    "    yearrets_rf_df = pd.DataFrame(yearrets_rf)\n",
    "    movingaverage = yearrets_rf_df.rolling(n_periods).mean()\n",
    "    movingstd = yearrets_rf_df.rolling(n_periods).std()\n",
    "    \n",
    "    sharperatios = movingaverage[0] / movingstd[0]\n",
    "    sharperatios = np.array(sharperatios)\n",
    "    \n",
    "    sharperatios = np.sqrt(12)*sharperatios\n",
    "        \n",
    "    return sharperatios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capmalphas(yearrets, n_periods):\n",
    "            \n",
    "    rols = RollingOLS(yearrets - rf, sm.add_constant(mkt_rf), window=n_periods)\n",
    "    results = rols.fit().params\n",
    "    \n",
    "    alphas = results['const']\n",
    "    alphas = np.array(alphas)\n",
    "        \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff3alphas(yearrets, n_periods):\n",
    "        \n",
    "    exog = pd.concat([mkt_rf, smb, hml], axis=1)\n",
    "    exog = sm.add_constant(exog)\n",
    "\n",
    "    rols = RollingOLS(yearrets - rf, exog, window=n_periods)\n",
    "    results = rols.fit().params\n",
    "        \n",
    "    alphas = results['const']\n",
    "    alphas = np.array(alphas)\n",
    "        \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS6_AzcLFJ4x"
   },
   "outputs": [],
   "source": [
    "def avgret(yearrets, startdate=19580131, enddate=20161230):\n",
    "    \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    nyears = len(yearrets[start_idx:end_idx])\n",
    "            \n",
    "    avgret = np.prod(1 + yearrets[start_idx:end_idx])**(1/nyears) - 1\n",
    "    \n",
    "    return avgret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS6_AzcLFJ4x"
   },
   "outputs": [],
   "source": [
    "def sharperatio(yearrets, startdate=19580131, enddate=20161230):\n",
    "    \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "    yearrets_rf = yearrets_rf[start_idx:end_idx]\n",
    "    \n",
    "    avg = np.mean(yearrets_rf)\n",
    "    std = np.std(yearrets_rf)\n",
    "    \n",
    "    sharperatio = avg / std\n",
    "    sharperatio = np.sqrt(12)*sharperatio\n",
    "        \n",
    "    return sharperatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capmalpha(yearrets, startdate=19580131, enddate=20161230):\n",
    "        \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "    yearrets_rf = yearrets_rf[start_idx:end_idx]\n",
    "    \n",
    "    exog = sm.add_constant(mkt_rf[start_idx:end_idx])\n",
    "    \n",
    "    capm = OLS(yearrets_rf, exog).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "    \n",
    "    alpha =  capm.params['const']\n",
    "    \n",
    "    t_alpha =  capm.tvalues['const']\n",
    "    \n",
    "    return alpha, t_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff3alpha(yearrets, startdate=19580131, enddate=20161230):\n",
    "        \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "    yearrets_rf = yearrets_rf[start_idx:end_idx]\n",
    "    \n",
    "    exog = pd.concat([mkt_rf[start_idx:end_idx], smb[start_idx:end_idx], hml[start_idx:end_idx]], axis=1)\n",
    "    exog = sm.add_constant(exog)\n",
    "    \n",
    "    ff3 = OLS(yearrets_rf, exog).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "    \n",
    "    alpha =  ff3.params['const']\n",
    "    \n",
    "    t_alpha =  ff3.tvalues['const']\n",
    "    \n",
    "    return alpha, t_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch4alpha(yearrets, startdate=19580131, enddate=20161230):\n",
    "        \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "    yearrets_rf = yearrets_rf[start_idx:end_idx]\n",
    "    \n",
    "    exog = pd.concat([mkt_rf[start_idx:end_idx], smb[start_idx:end_idx], hml[start_idx:end_idx], mom[start_idx:end_idx]], axis=1)\n",
    "    exog = sm.add_constant(exog)\n",
    "    \n",
    "    ch4 = OLS(yearrets_rf, exog).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "    \n",
    "    alpha =  ch4.params['const']\n",
    "    \n",
    "    t_alpha =  ch4.tvalues['const']\n",
    "    \n",
    "    return alpha, t_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff5alpha(yearrets, startdate=19580131, enddate=20161230):\n",
    "        \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "        \n",
    "    yearrets_rf = yearrets - rf.values\n",
    "    yearrets_rf = yearrets_rf[start_idx:end_idx]\n",
    "    \n",
    "    exog = pd.concat([mkt_rf[start_idx:end_idx], smb[start_idx:end_idx], hml[start_idx:end_idx],\n",
    "                      rmw[start_idx:end_idx], cma[start_idx:end_idx]], axis=1)\n",
    "    exog = sm.add_constant(exog)\n",
    "    \n",
    "    ff5 = OLS(yearrets_rf, exog).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        \n",
    "    alpha =  ff5.params['const']\n",
    "    \n",
    "    t_alpha =  ff5.tvalues['const']\n",
    "    \n",
    "    return alpha, t_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxLoss(yearrets, startdate=19580131, enddate=20161230):\n",
    "    \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "    \n",
    "    return np.min(yearrets[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxDD(compoundrets, startdate=19580131, enddate=20161230):\n",
    "    \n",
    "    start_idx = dates.tolist().index(startdate)\n",
    "    end_idx = dates.tolist().index(enddate)\n",
    "    \n",
    "    compoundrets = pd.DataFrame(compoundrets[start_idx:end_idx])\n",
    "    \n",
    "    Roll_Max = compoundrets.cummax()\n",
    "    Drawdown = compoundrets/Roll_Max - 1.0    \n",
    "    Max_Drawdown = np.min(Drawdown.values)\n",
    "        \n",
    "    return Max_Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnover(strategy, startdate=19580131, enddate=20161230):\n",
    "\n",
    "    strategy_t = strategy\n",
    "    strategy_t = strategy_t.loc[startdate:enddate]\n",
    "        \n",
    "    strategy_tp1 = strategy_t.shift(1)\n",
    "    \n",
    "    strategy_t = strategy_t.fillna(0)\n",
    "    strategy_tp1 = strategy_tp1.fillna(0)\n",
    "    \n",
    "    indturnover = abs(strategy_tp1.values - strategy_t.values * (1+returns.loc[startdate:enddate].values))    \n",
    "    monthlyturnover = np.nansum(indturnover, axis=1)\n",
    "        \n",
    "    monthlyturnover_pct = monthlyturnover / np.nansum(abs(strategy_t), axis=1)\n",
    "    monthlyturnover_pct[np.isinf(monthlyturnover_pct)] = 2\n",
    "        \n",
    "    avgturnover_pct = np.nanmean(monthlyturnover_pct)\n",
    "        \n",
    "    return avgturnover_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgyearrets_ew = np.nanmean(returns.to_numpy(), axis=1)\n",
    "avgexcessrets_ew = avgyearrets_ew - np.nanmean(returns.to_numpy(),axis=1)\n",
    "avgcompoundrets_ew = np.cumproduct(avgyearrets_ew+1)\n",
    "avgtotalret_ew = avgcompoundrets_ew[-1] - 1\n",
    "\n",
    "valueweightedindrets = np.multiply(returns.to_numpy(), mktcaps.to_numpy())\n",
    "avgyearrets = np.nansum(valueweightedindrets, axis=1) / np.nansum(mktcaps.to_numpy(), axis=1)\n",
    "avgcompoundrets = np.cumproduct(avgyearrets+1)\n",
    "avgtotalret = avgcompoundrets[-1] - 1\n",
    "\n",
    "\n",
    "def marketaveragesfrom(startdate):\n",
    "\n",
    "    avgar = avgret(avgyearrets, startdate, enddate=20161230)\n",
    "    avgar = '{:.2f}'.format(100*avgar)\n",
    "\n",
    "    avgsr = sharperatio(avgyearrets, startdate, enddate=20161230)\n",
    "    avgsr = '{:.2f}'.format(avgsr)\n",
    "\n",
    "    avgcapma, t_avgcapma = capmalpha(avgyearrets, startdate, enddate=20161230)\n",
    "    avgcapma, t_avgcapma = '{:.2f}'.format(100*avgcapma), '{:.2f}'.format(t_avgcapma)\n",
    "\n",
    "    avgff3a, t_avgff3a = ff3alpha(avgyearrets, startdate, enddate=20161230)\n",
    "    avgff3a, t_avgff3a = '{:.2f}'.format(100*avgff3a), '{:.2f}'.format(t_avgff3a)\n",
    "    \n",
    "    avgch4a, t_avgch4a = ch4alpha(avgyearrets, startdate, enddate=20161230)\n",
    "    avgch4a, t_avgch4a = '{:.2f}'.format(100*avgch4a), '{:.2f}'.format(t_avgch4a)\n",
    "    \n",
    "    avgff5a, t_avgff5a = ff5alpha(avgyearrets, startdate, enddate=20161230)\n",
    "    avgff5a, t_avgff5a = '{:.2f}'.format(100*avgff5a), '{:.2f}'.format(t_avgff5a)\n",
    "    \n",
    "    avgmaxloss = maxLoss(avgyearrets, startdate, enddate=20161230)\n",
    "    \n",
    "    avgmaxDD = maxDD(avgcompoundrets, startdate, enddate=20161230)\n",
    "    \n",
    "    return avgar, avgsr, avgcapma, t_avgcapma, avgff3a, t_avgff3a, avgch4a, t_avgch4a, avgff5a, t_avgff5a, avgmaxloss, avgmaxDD\n",
    "\n",
    "\n",
    "avgar58, avgsr58, avgcapma58, t_avgcapma58, avgff3a58, t_avgff3a58, avgch4a58, t_avgch4a58, avgff5a58, t_avgff5a58, avgmaxloss58, avgmaxDD58 = marketaveragesfrom(19580131)\n",
    "\n",
    "avgar58, avgsr80, avgcapma80, t_avgcapma80, avgff3a80, t_avgff3a80, avgch4a80, t_avgch4a80, avgff5a80, t_avgff5a80, avgmaxloss80, avgmaxDD80 = marketaveragesfrom(19800131)\n",
    "\n",
    "avgar58, avgsr00, avgcapma00, t_avgcapma00, avgff3a00, t_avgff3a00, avgch4a00, t_avgch4a00, avgff5a00, t_avgff5a00, avgmaxloss00, avgmaxDD00 = marketaveragesfrom(20000131)\n",
    "\n",
    "\n",
    "avgsharperatios_3y = sharperatios(avgyearrets, 12 * 3)\n",
    "avgsharperatios_10y = sharperatios(avgyearrets, 12 * 10)\n",
    "    \n",
    "avgcapmalphas_3y = capmalphas(avgyearrets, 12 * 3)\n",
    "avgcapmalphas_10y = capmalphas(avgyearrets, 12 * 10)\n",
    "\n",
    "avgff3alphas_3y = ff3alphas(avgyearrets, 12 * 3)\n",
    "avgff3alphas_10y = ff3alphas(avgyearrets, 12 * 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Decile Long Strategy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanseries = np.full(T, np.nan)\n",
    "yearrets_l10 = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_l10 = dict(zip(methods, [np.nan]*M))\n",
    "excessrets_l10 = dict(zip(methods,[nanseries] * M))\n",
    "totalret_l10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "yearrets_ew_l10 = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_ew_l10 = dict(zip(methods,[nanseries] * M))\n",
    "excessrets_ew_l10 = dict(zip(methods,[nanseries] * M))\n",
    "totalret_ew_l10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "sharperatios_3y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "sharperatios_10y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "    \n",
    "capmalphas_3y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "capmalphas_10y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "ff3alphas_3y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "ff3alphas_10y_l10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    print(method)\n",
    "    \n",
    "    strategy = longstrategy(method, 10)\n",
    "    \n",
    "    yearrets_l10[method], compoundrets_l10[method], excessrets_l10[method], totalret_l10[method] = strategyreturns(strategy)\n",
    "    \n",
    "    ewstrategy = longstrategy(method, 10, valueweighted=False)\n",
    "    yearrets_ew_l10[method], compoundrets_ew_l10[method], excessrets_ew_l10[method], totalret_ew_l10[method] = strategyreturns(ewstrategy)\n",
    "    \n",
    "    sharperatios_3y_l10[method] = sharperatios(yearrets_l10[method], 12 * 3)\n",
    "    sharperatios_10y_l10[method] = sharperatios(yearrets_l10[method], 12 * 10)\n",
    "    \n",
    "    capmalphas_3y_l10[method] = capmalphas(yearrets_l10[method], 12 * 3)\n",
    "    capmalphas_10y_l10[method] = capmalphas(yearrets_l10[method], 12 * 10)\n",
    "    \n",
    "    ff3alphas_3y_l10[method] = ff3alphas(yearrets_l10[method], 12 * 3)\n",
    "    ff3alphas_10y_l10[method] = ff3alphas(yearrets_l10[method], 12 * 10)\n",
    "    \n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):    \n",
    "\n",
    "        ar = avgret(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(longstrategy(method, 10), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets_l10[method], startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131)\n",
    "table.generate_tex('tables/l10_from58')\n",
    "\n",
    "table = tablefrom(19800131)\n",
    "table.generate_tex('tables/l10_from80')\n",
    "\n",
    "table = tablefrom(20000131)\n",
    "table.generate_tex('tables/l10_from00')\n",
    "\n",
    "print(\"Very nice!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally-weigthed compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS, Lasso, Ridge, Enet', '', '', '', 'PCR, PLS', '','Tree', '', 'GBRT', 'NN1, Forest', 'NN2, NN3', '', 'NN5, NN10', '']\n",
    "offsets = [0, 0, 0, 0, 5, 0, 0, 7, -12, 0, 0, 0, 3, 0]\n",
    "labeledlines(compoundrets_ew_l10, methodlabels_, offsets, ax)\n",
    "labeledline(avgcompoundrets, \"Market average\", 0, ax, width=2, color='k', style=':')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Equally-weighted Compound Returns - Top Decile Long Strategies\")\n",
    "plt.savefig(graphpath+'compoundrets_ew_l10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Value-weighted compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "offsets = [2, 0, 0, 0, 0, 0, 0, 0, 0, -8, 0, 0, 0, 0]\n",
    "methodlabels_ = ['OLS', 'Lasso, Ridge, Enet', '', '', 'PCR', 'PLS', '', 'Forest', 'GBRT, NN10', 'NN1', 'NN2', 'NN3', 'NN5, Tree', '']\n",
    "labeledlines(compoundrets_l10, methodlabels_, offsets, ax)\n",
    "labeledline(avgcompoundrets, \"Market average\", -3, ax, width=2, color='k', style=':')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Value-weighted Compound Returns - Top Decile Long Strategies\")\n",
    "plt.savefig(graphpath+'compoundrets_l10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ten-year Sharpe ratios:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS, Lasso, Ridge, Enet', '', '', '', 'PCR', 'PLS', 'Tree, GBRT', 'Forest', '', 'NN1', 'NN2', 'NN3', 'NN5, NN10', '']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, -5, 0, 0, 0, 0]\n",
    "labeledlines(sharperatios_10y_l10, methodlabels_, offsets, ax)\n",
    "labeledline(avgsharperatios_10y, \"Market average\", -3, ax, width=2, color='k', style=':')\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year Sharpe Ratios - Top Decile Long Strategies\")\n",
    "plt.savefig(graphpath+'sharperatios_10y_l10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ten-year FF3 alphas:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS, Lasso, Ridge, Enet', '', '', '', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5, NN10', '']\n",
    "offsets = [3, 0, 0, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(ff3alphas_10y_l10, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year FF3 Alphas - Top Decile Long Strategies\")\n",
    "plt.savefig(graphpath+'ff3alphas_10y_l10.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top/Bottom Decile Long/Short Strategy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanseries = np.full(T, np.nan)\n",
    "yearrets_ls10 = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "excessrets_ls10 = dict(zip(methods,[nanseries] * M))\n",
    "totalret_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "yearrets_ew_ls10 = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_ew_ls10 = dict(zip(methods,[nanseries] * M))\n",
    "excessrets_ew_ls10 = dict(zip(methods,[nanseries] * M))\n",
    "totalret_ew_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "sharperatios_3y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "sharperatios_10y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "    \n",
    "capmalphas_3y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "capmalphas_10y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "ff3alphas_3y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "ff3alphas_10y_ls10 = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    strategy = longshortstrategy(method, 10)\n",
    "    \n",
    "    yearrets_ls10[method], compoundrets_ls10[method], excessrets_ls10[method], totalret_ls10[method] = strategyreturns(strategy)\n",
    "    \n",
    "    ewstrategy = longshortstrategy(method, 10, valueweighted=False)\n",
    "    yearrets_ew_ls10[method], compoundrets_ew_ls10[method], excessrets_ew_ls10[method], totalret_ew_ls10[method] = strategyreturns(ewstrategy)\n",
    "    \n",
    "    sharperatios_3y_ls10[method] = sharperatios(yearrets_ls10[method], 12 * 3)\n",
    "    sharperatios_10y_ls10[method] = sharperatios(yearrets_ls10[method], 12 * 10)\n",
    "    \n",
    "    capmalphas_3y_ls10[method] = capmalphas(yearrets_ls10[method], 12 * 3)\n",
    "    capmalphas_10y_ls10[method] = capmalphas(yearrets_ls10[method], 12 * 10)\n",
    "    \n",
    "    ff3alphas_3y_ls10[method] = ff3alphas(yearrets_ls10[method], 12 * 3)\n",
    "    ff3alphas_10y_ls10[method] = ff3alphas(yearrets_ls10[method], 12 * 10)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):    \n",
    "\n",
    "        ar = avgret(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(longshortstrategy(method, 10), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets_ls10[method], startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131)\n",
    "table.generate_tex('tables/ls10_from58')\n",
    "\n",
    "table = tablefrom(19800131)\n",
    "table.generate_tex('tables/ls10_from80')\n",
    "\n",
    "table = tablefrom(20000131)\n",
    "table.generate_tex('tables/ls10_from00')\n",
    "\n",
    "print(\"Very nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally-weigthed compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(compoundrets_ew_ls10, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Equally-weighted Compound Returns - Top/Bottom Decile Long/Short Strategies\")\n",
    "plt.savefig(graphpath+'compoundrets_ew_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Value-weighted compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(compoundrets_ls10, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Value-weighted Compound Returns - Top/Bottom Decile Long/Short Strategies\")\n",
    "plt.savefig(graphpath+'compoundrets_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ten-year Sharpe ratios:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(sharperatios_10y_ls10, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year Sharpe Ratios - Top/Bottom Decile Long/Short Strategies\")\n",
    "plt.savefig(graphpath+'sharperatios_10y_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ten-year FF3 alphas:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(ff3alphas_10y_ls10, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year FF3 Alphas - Top/Bottom Decile Long/Short Strategies\")\n",
    "plt.savefig(graphpath+'ff3alphas_10y_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-weighted Long/Short Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanseries = np.full(T, np.nan)\n",
    "yearrets_rank = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_rank = dict(zip(methods, [np.nan]*M))\n",
    "excessrets_rank = dict(zip(methods,[nanseries] * M))\n",
    "totalret_rank = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "yearrets_ew_rank = dict(zip(methods,[nanseries] * M))\n",
    "compoundrets_ew_rank = dict(zip(methods,[nanseries] * M))\n",
    "excessrets_ew_rank = dict(zip(methods,[nanseries] * M))\n",
    "totalret_ew_rank = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "sharperatios_3y_rank = dict(zip(methods, [np.nan]*M))\n",
    "sharperatios_10y_rank = dict(zip(methods, [np.nan]*M))\n",
    "    \n",
    "capmalphas_3y_rank = dict(zip(methods, [np.nan]*M))\n",
    "capmalphas_10y_rank = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "ff3alphas_3y_rank = dict(zip(methods, [np.nan]*M))\n",
    "ff3alphas_10y_rank = dict(zip(methods, [np.nan]*M))\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    strategy = rankstrategy(method)\n",
    "    yearrets_rank[method], compoundrets_rank[method], excessrets_rank[method], totalret_rank[method] = strategyreturns(strategy)\n",
    "    \n",
    "    ewstrategy = rankstrategy(method, valueweighted=False)\n",
    "    yearrets_ew_rank[method], compoundrets_ew_rank[method], excessrets_ew_rank[method], totalret_ew_rank[method] = strategyreturns(ewstrategy)\n",
    "    \n",
    "    sharperatios_3y_rank[method] = sharperatios(yearrets_rank[method], 12 * 3)\n",
    "    sharperatios_10y_rank[method] = sharperatios(yearrets_rank[method], 12 * 10)\n",
    "    \n",
    "    capmalphas_3y_rank[method] = capmalphas(yearrets_rank[method], 12 * 3)\n",
    "    capmalphas_10y_rank[method] = capmalphas(yearrets_rank[method], 12 * 10)\n",
    "    \n",
    "    ff3alphas_3y_rank[method] = ff3alphas(yearrets_rank[method], 12 * 3)\n",
    "    ff3alphas_10y_rank[method] = ff3alphas(yearrets_rank[method], 12 * 10)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):    \n",
    "\n",
    "        ar = avgret(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(rankstrategy(method), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets_rank[method], startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131)\n",
    "table.generate_tex('tables/rank_from58')\n",
    "\n",
    "print(\"Very nice!\")\n",
    "\n",
    "table = tablefrom(19800131)\n",
    "table.generate_tex('tables/rank_from80')\n",
    "\n",
    "print(\"Very nice!!\")\n",
    "\n",
    "table = tablefrom(20000131)\n",
    "table.generate_tex('tables/rank_from00')\n",
    "\n",
    "print(\"Very nice!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally-weigthed compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(compoundrets_ew_rank, methodlabels_, offsets, ax)\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Equally-weighted Compound Returns - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'compoundrets_ew_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Value-weighted compound returns:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(compoundrets_rank, methodlabels_, offsets, ax)\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Value-weighted Compound Returns - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'compoundrets_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ten-year Sharpe ratios:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(sharperatios_10y_rank, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year Sharpe Ratios - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'sharperatios_10y_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Ten-year FF3 alphas:\n",
    "fig, ax = basefig(8,3)\n",
    "methodlabels_ = ['OLS', 'Lasso', 'Ridge', 'Enet', 'PCR', 'PLS', 'Tree', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3', 'NN5', 'NN10']\n",
    "offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "labeledlines(ff3alphas_10y_rank, methodlabels_, offsets, ax)\n",
    "plt.plot(dtdates, np.zeros_like(dates), 'k')\n",
    "plt.title(\"10-Year FF3 Alphas - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'ff3alphas_10y_rank.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness check: restricting the universe to Top 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mktcaps.to_numpy()\n",
    "\n",
    "mktcaplims_top1000 = []\n",
    "mktcaplims_top100 = []\n",
    "\n",
    "for date in dates:\n",
    "    \n",
    "    mktcaps_date = mktcaps.loc[date].to_numpy()\n",
    "    mktcaps_date = mktcaps_date[~np.isnan(mktcaps_date)]\n",
    "        \n",
    "    mktcaplim_top1000 = np.partition(mktcaps_date, -1000)[-1000]\n",
    "    mktcaplims_top1000 = np.append(mktcaplims_top1000, mktcaplim_top1000)\n",
    "    \n",
    "    mktcaplim_top100 = np.partition(mktcaps_date, -100)[-100]\n",
    "    mktcaplims_top100 = np.append(mktcaplims_top100, mktcaplim_top100)\n",
    "\n",
    "mask_top1000 = (mktcaps.ge(mktcaplims_top1000, axis=0))\n",
    "mask_top100 = (mktcaps.ge(mktcaplims_top100, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longstrategy_subset(method, percentile, valueweighted=True, mask=mask_top1000):\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy().to_numpy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "              \n",
    "    prediction = predictions[method].copy()\n",
    "    \n",
    "    p_top = np.nanpercentile(prediction[mask], 100 - percentile, axis=1, keepdims=True)\n",
    "    \n",
    "    strategy[prediction[mask]>=p_top] = 1\n",
    "        \n",
    "    if valueweighted: strategy = strategy * mktcaps.values\n",
    "            \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True)\n",
    "        \n",
    "    strategy = strategy / rescaler\n",
    "        \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "        \n",
    "    return strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OePekbBE78of"
   },
   "outputs": [],
   "source": [
    "def longshortstrategy_subset(method, percentile, valueweighted=True, mask=mask_top1000):\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy().to_numpy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "              \n",
    "    prediction = predictions[method].copy()\n",
    "        \n",
    "    p_top = np.nanpercentile(prediction[mask], 100 - percentile, axis=1, keepdims=True)\n",
    "    p_bottom = np.nanpercentile(prediction[mask], percentile, axis=1, keepdims=True)\n",
    "\n",
    "    strategy[prediction[mask]>p_top] = 1 \n",
    "\n",
    "    strategy[prediction[mask]<p_bottom] = -1\n",
    "    \n",
    "    if valueweighted: strategy = strategy * mktcaps.values\n",
    "            \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True) / 2\n",
    "    strategy = strategy / rescaler\n",
    "    \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K--DQAUiFJ4V"
   },
   "outputs": [],
   "source": [
    "def rankstrategy_subset(method, valueweighted=True, mask=mask_top1000):\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    strategy = returns.copy().to_numpy()\n",
    "    strategy[~np.isnan(strategy)] = 0\n",
    "          \n",
    "    prediction = predictions[method].copy().to_numpy()\n",
    "    prediction[np.isnan(prediction)] = np.inf\n",
    "    \n",
    "    prediction_order = np.argsort(prediction, axis=1)\n",
    "    prediction_rank = np.argsort(prediction_order, axis=1) + 1\n",
    "    \n",
    "    numberofstocks = np.count_nonzero(~np.isnan(strategy), axis=1)\n",
    "    \n",
    "    strategy = (prediction_rank.T - ((numberofstocks+1)/2))     \n",
    "    strategy =  strategy.T\n",
    "    \n",
    "    strategy[np.isnan(returns.values)] = np.nan\n",
    "        \n",
    "    if valueweighted: strategy = strategy * mktcaps.values\n",
    "        \n",
    "    strategy[mask] = 0 #this is cheating a bit...\n",
    "            \n",
    "    rescaler = np.nansum(abs(strategy), axis=1, keepdims=True) / 2\n",
    "    strategy = strategy / rescaler\n",
    "        \n",
    "    strategy = pd.DataFrame(strategy)\n",
    "    strategy.index, strategy.columns = returns.index, returns.columns\n",
    "    \n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate, mask):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):\n",
    "                \n",
    "        strategy = longstrategy_subset(method, 10, mask=mask)\n",
    "            \n",
    "        yearrets, compoundrets, excessrets, totalret = strategyreturns(strategy)\n",
    "        \n",
    "        ar = avgret(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets, startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(longstrategy_subset(method, 10, mask=mask), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top1000)\n",
    "table.generate_tex('tables/l10_top100')\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top100)\n",
    "table.generate_tex('tables/l10_top1000')\n",
    "\n",
    "print(\"Very nice!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate, mask):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):\n",
    "                \n",
    "        strategy = longshortstrategy_subset(method, 10, mask=mask)\n",
    "            \n",
    "        yearrets, compoundrets, excessrets, totalret = strategyreturns(strategy)\n",
    "        \n",
    "        ar = avgret(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets, startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(longshortstrategy_subset(method, 10, mask=mask), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top1000)\n",
    "table.generate_tex('tables/ls10_top1000')\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top100)\n",
    "table.generate_tex('tables/ls10_top100')\n",
    "\n",
    "print(\"Very nice!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefrom(startdate, mask):\n",
    "    \n",
    "    table = Tabular('lccccccccccccc')\n",
    "\n",
    "    table.add_row(('', MultiColumn(13, align='c', data=str(int(startdate/10000))+'-2016')))\n",
    "    table.add_hline(2, 14)\n",
    "\n",
    "    table.add_row(('', 'Avg.', 'Sharpe', 'Avg.', 'Max.', 'Max.',\n",
    "                    'CAPM-', MultiRow(2, data='(t-stat)'), 'FF3-', MultiRow(2, data='(t-stat)'),\n",
    "                    'CH4-', MultiRow(2, data='(t-stat)'), 'FF5-', MultiRow(2, data='(t-stat)')))\n",
    "    table.add_row(('', 'Ret.', 'Ratio', 'Turn.', 'Loss', 'DD', 'alpha', '', 'alpha', '', 'alpha', '', 'alpha', ''))\n",
    "\n",
    "    for (i,method) in enumerate(methods):\n",
    "                \n",
    "        strategy = rankstrategy_subset(method, mask=mask)\n",
    "            \n",
    "        yearrets, compoundrets, excessrets, totalret = strategyreturns(strategy)\n",
    "        \n",
    "        ar = avgret(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ar = '{:.2f}'.format(100*ar)\n",
    "\n",
    "        sr = sharperatio(yearrets, startdate=startdate, enddate=20161230)\n",
    "        sr = '{:.2f}'.format(sr)\n",
    "        \n",
    "        Max_Loss = maxLoss(yearrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Loss = '{:.1f}'.format(100*Max_Loss)\n",
    "        \n",
    "        Max_Drawdown = maxDD(compoundrets, startdate=startdate, enddate=20161230)\n",
    "        Max_Drawdown = '{:.1f}'.format(100*Max_Drawdown)\n",
    "        \n",
    "        Avg_Turnover = turnover(rankstrategy_subset(method, 10, mask=mask), startdate=startdate, enddate=20161230)\n",
    "        Avg_Turnover = '{:.0f}'.format(100*Avg_Turnover)\n",
    "\n",
    "        capma, t_capma = capmalpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        capma, t_capma = '{:.2f}'.format(100*capma), '({:.2f})'.format(t_capma)\n",
    "\n",
    "        ff3a, t_ff3a = ff3alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff3a, t_ff3a = '{:.2f}'.format(100*ff3a), '({:.2f})'.format(t_ff3a)\n",
    "        \n",
    "        ch4a, t_ch4a = ch4alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ch4a, t_ch4a = '{:.2f}'.format(100*ch4a), '({:.2f})'.format(t_ch4a)\n",
    "        \n",
    "        ff5a, t_ff5a = ff5alpha(yearrets, startdate=startdate, enddate=20161230)\n",
    "        ff5a, t_ff5a = '{:.2f}'.format(100*ff5a), '({:.2f})'.format(t_ff5a)\n",
    "\n",
    "        table.add_row((methodlabels[i], ar, sr, Avg_Turnover, Max_Loss, Max_Drawdown, capma, t_capma, ff3a, t_ff3a, ch4a, t_ch4a, ff5a, t_ff5a))\n",
    "        \n",
    "    return table\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top1000)\n",
    "table.generate_tex('tables/rank_top1000')\n",
    "\n",
    "table = tablefrom(19580131, mask=mask_top100)\n",
    "table.generate_tex('tables/rank_top100')\n",
    "\n",
    "print(\"Very nice!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness check: transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computemonthlyturnover(strategy):\n",
    "\n",
    "    strategy_t = strategy\n",
    "    strategy_tp1 = strategy_t.shift(1)\n",
    "\n",
    "    strategy_t = strategy_t.fillna(0)\n",
    "    strategy_tp1 = strategy_tp1.fillna(0)\n",
    "\n",
    "    indturnover = abs((strategy_tp1.values - strategy_t.values * (1+returns.values)))\n",
    "\n",
    "    monthlyturnover = np.nansum(indturnover, axis=1) / np.nansum(abs(strategy_t), axis=1)\n",
    "    monthlyturnover[np.isinf(monthlyturnover)] = 2\n",
    "\n",
    "    return monthlyturnover\n",
    "\n",
    "def returnsaftercosts(yearrets, monthlyturnover, commission):\n",
    "    \n",
    "    yearrets_tradingcosts = yearrets - commission * monthlyturnover\n",
    "\n",
    "    compoundrets_tradingcosts = np.cumprod(1 + yearrets_tradingcosts)\n",
    "    \n",
    "    totalret_tradingscosts = compoundrets_tradingcosts[-1]\n",
    "    \n",
    "    avgret_tradingscosts = totalret_tradingscosts**(1/T) - 1\n",
    "    \n",
    "    return yearrets_tradingcosts, compoundrets_tradingcosts, totalret_tradingscosts, avgret_tradingscosts\n",
    "\n",
    "\n",
    "avgrets_tradingscosts = np.array([])\n",
    "commissions = np.linspace(0, 0.1, 10)\n",
    "\n",
    "costmethods = fewmethods\n",
    "offsets = [0, 0, 0, 0, 3, -3]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax_ in ax:\n",
    "    ax_.spines['right'].set_visible(False)\n",
    "    ax_.spines['top'].set_visible(False)\n",
    "\n",
    "for (i,method) in enumerate(costmethods):\n",
    "\n",
    "    strategy = longstrategy(method, 10)\n",
    "    \n",
    "    yearrets, compoundrets, _, _ = strategyreturns(strategy)\n",
    "\n",
    "    monthlyturnover = computemonthlyturnover(strategy)\n",
    "    \n",
    "    avgrets_tradingscosts = np.array([])\n",
    "    alphas_tradingscosts = np.array([])\n",
    "\n",
    "    for commission in commissions:\n",
    "\n",
    "        yearrets_tradingcosts, _ , _ , avgret_tradingscosts = returnsaftercosts(yearrets, monthlyturnover, commission)\n",
    "        alpha_tradingscosts = ff3alpha(yearrets_tradingcosts)[0]\n",
    "        \n",
    "        avgrets_tradingscosts = np.append(avgrets_tradingscosts, avgret_tradingscosts)\n",
    "        alphas_tradingscosts = np.append(alphas_tradingscosts, alpha_tradingscosts)\n",
    "        \n",
    "    ax[0].plot(commissions, avgrets_tradingscosts)\n",
    "    ax[0].annotate(s=fewmethodlabels[i], xy=(commissions[-1], avgrets_tradingscosts[-1]), xytext=(5,offsets[i]), textcoords='offset points', va='center')\n",
    "    \n",
    "    ax[1].plot(commissions, alphas_tradingscosts)\n",
    "    ax[1].annotate(s=fewmethodlabels[i], xy=(commissions[-1], alphas_tradingscosts[-1]), xytext=(5,0), textcoords='offset points', va='center')\n",
    "        \n",
    "        \n",
    "ax[0].plot(commissions, np.zeros_like(commissions), 'k', linewidth=1)\n",
    "ax[0].set_ylabel('Average monthly return, adjusted for trading costs')\n",
    "ax[0].set_xlabel('Trading fees: commission in percent')\n",
    "ax[0].set_title('Returns')\n",
    "ax[0].xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax[0].set_ylim([-0.14, 0.075])\n",
    "\n",
    "\n",
    "ax[1].plot(commissions, np.zeros_like(commissions), 'k', linewidth=1)\n",
    "ax[1].set_ylabel('Average monthly alpha, adjusted for trading costs')\n",
    "ax[1].set_xlabel('Trading fees: commission in percent')\n",
    "ax[1].set_title('Alphas')\n",
    "ax[1].xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax[1].set_ylim([-0.14, 0.075])\n",
    "\n",
    "plt.tight_layout(pad=1)\n",
    "plt.savefig(graphpath+'tradingcosts.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Volume and Short Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation, exploratory graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nanshares = np.mean(~np.isnan(returns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trmratio'] = df['cshtrm'] / df['shrout']\n",
    "trmratios = df[['date','permno','trmratio',]].pivot(index='date', columns='permno', values='trmratio')\n",
    "\n",
    "trmratios_i = trmratios.copy()\n",
    "\n",
    "trmratios_i = trmratios.interpolate(method='linear', limit=3)\n",
    "trmratios_i[np.isnan(returns)] = np.nan\n",
    "\n",
    "trmratios = trmratios.clip(trmratios.quantile(0.001, axis=1), trmratios.quantile(0.999, axis=1), axis=0)\n",
    "trmratios_i = trmratios_i.clip(trmratios_i.quantile(0.001, axis=1), trmratios_i.quantile(0.999, axis=1), axis=0)\n",
    "\n",
    "\n",
    "trmratios_obs = np.count_nonzero(~np.isnan(trmratios), axis=1)\n",
    "trmratios_i_obs = np.count_nonzero(~np.isnan(trmratios_i), axis=1)\n",
    "\n",
    "trmratios_nanshares = np.mean(~np.isnan(trmratios), axis=1)\n",
    "trmratios_missingshares = (return_nanshares - trmratios_nanshares) / return_nanshares\n",
    "\n",
    "trmratios_i_nanshares = np.mean(~np.isnan(trmratios_i), axis=1)\n",
    "trmratios_i_missingshares = (return_nanshares - trmratios_i_nanshares) / return_nanshares\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "ax[0].plot(dtdates, trmratios_obs, 'o', alpha=0.1, markersize=6, label='Original data')\n",
    "ax[0].plot(dtdates, trmratios_i_obs, 'C2', linewidth=1, label='Interpolation')\n",
    "ax[0].legend(loc='upper left', frameon=False)\n",
    "ax[0].set_title(\"Number of Observations\", pad=10)\n",
    "\n",
    "ax[1].plot(dtdates, trmratios_missingshares, 'o', alpha=0.1, markersize=6)\n",
    "ax[1].plot(dtdates, trmratios_i_missingshares, 'C2', linewidth=1)\n",
    "ax[1].set_title(\"Share of Missing Observations\", pad=10)\n",
    "\n",
    "ax[2].plot(dtdates, trmratios[59408], 'o', markersize=6, label='Original data')\n",
    "ax[2].plot(dtdates, trmratios_i[59408], 'C2', linewidth=1)\n",
    "ax[2].plot(dtdates[np.isnan(trmratios[59408])], trmratios_i[59408][np.isnan(trmratios[59408])], 'C2o', markersize=6, label='Interpolation')\n",
    "ax[2].legend(loc='lower right', frameon=False)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_title(\"Interpolation Example : Ford\", pad=10)\n",
    "\n",
    "fig.suptitle(\"Trading Ratio : Monthly Volume / Shares Outstanding\", size=18, y=1.1)\n",
    "plt.savefig(graphpath+'datanalysis_trmratios.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shortratio'] = df['shortint'] / df['shrout']\n",
    "shortratios = df[['date','permno','shortratio',]].pivot(index='date', columns='permno', values='shortratio')\n",
    "\n",
    "\n",
    "shortratios_i = shortratios.interpolate(method='linear', limit=3)\n",
    "shortratios_i[np.isnan(returns)] = np.nan\n",
    "\n",
    "shortratios = shortratios.clip(shortratios.quantile(0.001, axis=1), shortratios.quantile(0.999, axis=1), axis=0)\n",
    "shortratios_i = shortratios_i.clip(shortratios_i.quantile(0.001, axis=1), shortratios_i.quantile(0.999, axis=1), axis=0)\n",
    "\n",
    "shortratios_obs = np.count_nonzero(~np.isnan(shortratios), axis=1)\n",
    "shortratios_i_obs = np.count_nonzero(~np.isnan(shortratios_i), axis=1)\n",
    "\n",
    "shortratios_nanshares = np.mean(~np.isnan(shortratios), axis=1)\n",
    "shortratios_missingshares = (return_nanshares - shortratios_nanshares) / return_nanshares\n",
    "\n",
    "shortratios_i_nanshares = np.mean(~np.isnan(shortratios_i), axis=1)\n",
    "shortratios_i_missingshares = (return_nanshares - shortratios_i_nanshares) / return_nanshares\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "ax[0].plot(dtdates, shortratios_obs, 'o', alpha=0.1, markersize=6, label='Original data')\n",
    "ax[0].plot(dtdates, shortratios_i_obs, 'C2', linewidth=1, label='Interpolation')\n",
    "ax[0].legend(loc='upper left', frameon=False)\n",
    "ax[0].set_title(\"Number of Observations\", pad=10)\n",
    "\n",
    "ax[1].plot(dtdates, shortratios_missingshares, 'o', alpha=0.1, markersize=6)\n",
    "ax[1].plot(dtdates, shortratios_i_missingshares, 'C2', linewidth=1)\n",
    "ax[1].set_title(\"Share of Missing Observations\", pad=10)\n",
    "\n",
    "ax[2].plot(dtdates, shortratios[59408], 'o', markersize=6, label='Original data')\n",
    "ax[2].plot(dtdates, shortratios_i[59408], 'C2', linewidth=1)\n",
    "ax[2].plot(dtdates[np.isnan(shortratios[59408])], shortratios_i[59408][np.isnan(shortratios[59408])], 'C2o', markersize=6, label='Interpolation')\n",
    "ax[2].legend(loc='lower right', frameon=False)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_title(\"Interpolation Example : Ford\", pad=10)\n",
    "\n",
    "fig.suptitle(\"Short Ratio : Short Interest / Shares Outstanding\", size=18, y=1.1)\n",
    "plt.savefig(graphpath+'datanalysis_shortratios.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daystocover'] = df['shortint'] / df['cshtrm'] / 21\n",
    "daystocovers = df[['date','permno','daystocover',]].pivot(index='date', columns='permno', values='daystocover')\n",
    "\n",
    "daystocovers_i = daystocovers.interpolate(method='linear', limit=3)\n",
    "daystocovers_i[np.isnan(returns)] = np.nan\n",
    "\n",
    "daystocovers = daystocovers.clip(daystocovers.quantile(0.001, axis=1), daystocovers.quantile(0.999, axis=1), axis=0)\n",
    "daystocovers_i = daystocovers_i.clip(daystocovers_i.quantile(0.001, axis=1), daystocovers_i.quantile(0.999, axis=1), axis=0)\n",
    "\n",
    "daystocovers_obs = np.count_nonzero(~np.isnan(daystocovers), axis=1)\n",
    "daystocovers_i_obs = np.count_nonzero(~np.isnan(daystocovers_i), axis=1)\n",
    "\n",
    "daystocovers_nanshares = np.mean(~np.isnan(daystocovers), axis=1)\n",
    "daystocovers_missingshares = (return_nanshares - daystocovers_nanshares) / return_nanshares\n",
    "\n",
    "daystocovers_i_nanshares = np.mean(~np.isnan(daystocovers_i), axis=1)\n",
    "daystocovers_i_missingshares = (return_nanshares - daystocovers_i_nanshares) / return_nanshares\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "ax[0].plot(dtdates, daystocovers_obs, 'o', alpha=0.1, markersize=6, label='Original data')\n",
    "ax[0].plot(dtdates, daystocovers_i_obs, 'C2', linewidth=1, label='Interpolation')\n",
    "ax[0].legend(loc='upper left', frameon=False)\n",
    "ax[0].set_title(\"Number of Observations\", pad=10)\n",
    "\n",
    "ax[1].plot(dtdates, daystocovers_missingshares, 'o', alpha=0.1, markersize=6)\n",
    "ax[1].plot(dtdates, daystocovers_i_missingshares, 'C2', linewidth=1)\n",
    "ax[1].set_title(\"Share of Missing Observations\", pad=10)\n",
    "\n",
    "ax[2].plot(dtdates, daystocovers[59408], 'o', markersize=6, label='Original data')\n",
    "ax[2].plot(dtdates, daystocovers_i[59408], 'C2', linewidth=1)\n",
    "ax[2].plot(dtdates[np.isnan(daystocovers[59408])], daystocovers_i[59408][np.isnan(daystocovers[59408])], 'C2o', markersize=6, label='Interpolation')\n",
    "ax[2].legend(loc='lower right', frameon=False)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_title(\"Interpolation Example : Ford\", pad=10)\n",
    "\n",
    "fig.suptitle(\"Days to Cover : Short Interest / Monthly Volume\", size=18, y=1.1)\n",
    "plt.savefig(graphpath+'datanalysis_daystocovers.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortratios_pctile = 100 * shortratios.rank(pct=True, axis=1)\n",
    "daystocovers_pctile = 100 * daystocovers.rank(pct=True, axis=1)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading activity and short interest ratios around ML-Portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategytrmratios(strategy):\n",
    "    \n",
    "    indtrmratios = np.multiply(strategy.values, trmratios_i.values)\n",
    "    strategy_trmratios = np.nansum(indtrmratios, axis=1)\n",
    "\n",
    "    return strategy_trmratios\n",
    "\n",
    "def strategyshortratios(strategy):\n",
    "    \n",
    "    indshortratios = np.multiply(strategy.values, shortratios_i.values)\n",
    "    strategy_shortratios = np.nansum(indshortratios, axis=1)\n",
    "\n",
    "    return strategy_shortratios\n",
    "\n",
    "def strategydaystocovers(strategy):\n",
    "    \n",
    "    strategy_shortside = 2 * (strategy<0).astype('int')  # 2 * so that weights sum to 1\n",
    "    daystocovers_shortside = np.multiply(strategy_shortside.values, daystocovers_i.values)\n",
    "    inddaystocovers = np.multiply(-strategy.values, daystocovers_shortside)\n",
    "    strategy_daystocovers = np.nansum(inddaystocovers, axis=1)\n",
    "\n",
    "    return strategy_daystocovers\n",
    "\n",
    "def strategydaystocovers_max(strategy):\n",
    "    \n",
    "    strategy_shortside = (strategy<0).astype('int')\n",
    "    daystocovers_shortside = np.multiply(strategy_shortside.values, daystocovers_i.values)\n",
    "    strategy_daystocovers_max = np.nanmax(daystocovers_shortside, axis=1)\n",
    "\n",
    "    return strategy_daystocovers_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top/bottom decile strategies\n",
    "\n",
    "nanseries = np.full(T, np.nan)\n",
    "trmratios_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "shortratios_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "daystocovers_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "daystocovers_max_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "\n",
    "for method in fewmethods:\n",
    "    \n",
    "    strategy = longshortstrategy(method, 10)\n",
    "    \n",
    "    trmratios_ls10[method] = strategytrmratios(strategy)\n",
    "    shortratios_ls10[method] = strategyshortratios(strategy)\n",
    "    daystocovers_ls10[method] = strategydaystocovers(strategy)\n",
    "    daystocovers_max_ls10[method] = strategydaystocovers_max(strategy)\n",
    "\n",
    "print(\"All done!\")\n",
    "\n",
    "# Trading ratios:\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, trmratios_ls10[method], color=fewmethodcolors[i], alpha=0.2)\n",
    "    ax.plot(dtdates, ma(trmratios_ls10[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(-10**-3, 10**-3)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Trading Ratios - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'trmratios_i_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Short ratios:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, shortratios_ls10[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(shortratios_ls10[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(2*-10**-4, 2*10**-4)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Short Ratios - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'shortratios_i_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Days to cover, weighted average:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, daystocovers_ls10[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(daystocovers_ls10[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "plt.ylim(0, 3)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Days to Cover - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'daystocovers_i_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Days to cover, max:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, daystocovers_max_ls10[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(daystocovers_max_ls10[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Maximum Days to Cover - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'daystocovers_max_i_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-weighted strategies\n",
    "\n",
    "\n",
    "nanseries = np.full(T, np.nan)\n",
    "trmratios_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "shortratios_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "daystocovers_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "daystocovers_max_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "\n",
    "for method in fewmethods:\n",
    "    \n",
    "    strategy = rankstrategy(method)\n",
    "    \n",
    "    trmratios_rank[method] = strategytrmratios(strategy)\n",
    "    shortratios_rank[method] = strategyshortratios(strategy)\n",
    "    daystocovers_rank[method] = strategydaystocovers(strategy)\n",
    "    daystocovers_max_rank[method] = strategydaystocovers_max(strategy)\n",
    "\n",
    "print(\"All done!\")\n",
    "\n",
    "# Trading ratios:\n",
    "fig, ax = basefig(10,5)\n",
    "offsets = [-15, +5, +15, 0, 0, 0]\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, trmratios_rank[method], color=fewmethodcolors[i], alpha=0.2)\n",
    "    ax.plot(dtdates, ma(trmratios_rank[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(-10**-4, 10**-4)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Trading Ratios - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'trmratios_i_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Short ratios:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "offsets = [-15, +5, +15, 0, 0, 0]\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, shortratios_rank[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(shortratios_rank[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(3*-10**-5, 3*10**-5)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Short Ratios - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'shortratios_i_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Days to cover, weighted average:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, daystocovers_rank[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(daystocovers_rank[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "plt.ylim(0, 3)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Weighted Days to Cover - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'daystocovers_i_rank.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Days to cover, max:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, daystocovers_max_rank[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(daystocovers_max_rank[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "plt.ylim(0, 200)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Maximum Days to Cover - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'daystocovers_max_i_rank.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness test: using the non-interpolated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategytrmratios(strategy):\n",
    "    \n",
    "    indtrmratios = np.multiply(strategy.values, trmratios.values)\n",
    "    strategy_trmratios = np.nansum(indtrmratios, axis=1)\n",
    "\n",
    "    return strategy_trmratios\n",
    "\n",
    "def strategyshortratios(strategy):\n",
    "    \n",
    "    indshortratios = np.multiply(strategy.values, shortratios.values)\n",
    "    strategy_shortratios = np.nansum(indshortratios, axis=1)\n",
    "\n",
    "    return strategy_shortratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top/bottom decile strategies\n",
    "\n",
    "nanseries = np.full(T, np.nan)\n",
    "trmratios_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "shortratios_ls10 = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "\n",
    "for method in fewmethods:\n",
    "    \n",
    "    strategy = longshortstrategy(method, 10)\n",
    "\n",
    "    shortratios_ls10[method] = strategyshortratios(strategy)\n",
    "\n",
    "print(\"All done!\")\n",
    "\n",
    "# Short ratios:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "offsets = [-15, +5, +15, 0, 0, 0]\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, shortratios_ls10[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(shortratios_ls10[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(2*-10**-4, 2*10**-4)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Value-weighted Short Ratios - Top/Bottom Decile Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'shortratios_ls10.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-weighted strategies\n",
    "\n",
    "\n",
    "nanseries = np.full(T, np.nan)\n",
    "trmratios_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "shortratios_rank = dict(zip(fewmethods,[nanseries] * Mfew))\n",
    "\n",
    "for method in fewmethods:\n",
    "    \n",
    "    strategy = rankstrategy(method)\n",
    "\n",
    "    shortratios_rank[method] = strategyshortratios(strategy)\n",
    "\n",
    "print(\"All done!\")\n",
    "\n",
    "# Short ratios:\n",
    "\n",
    "fig, ax = basefig(10,5)\n",
    "offsets = [-15, +5, +15, 0, 0, 0]\n",
    "\n",
    "for (i,method) in enumerate(fewmethods):\n",
    "    \n",
    "    ax.plot(dtdates, shortratios_rank[method], color=fewmethodcolors[i], alpha=0.1)\n",
    "    ax.plot(dtdates, ma(shortratios_rank[method], centered=True), color=fewmethodcolors[i], label=fewmethodlabels[i]+\" (mov. avg.)\")\n",
    "\n",
    "ax.plot(dtdates, np.zeros(len(dtdates)), 'k-')\n",
    "\n",
    "plt.xlim(left=datetime.date(1980, 1, 1))\n",
    "#plt.ylim(3*-10**-5, 3*10**-5)\n",
    "plt.legend(ncol=3, frameon=False)\n",
    "plt.title(\"Value-weighted Short Ratios - Rank-weighted Long/Short Strategy\")\n",
    "plt.savefig(graphpath+'shortratios_rank.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Activity and Short Interest around Machine Learning Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trmratiovsprediction_tsanalysis(method, methodtitlelabel):\n",
    "\n",
    "    slopes = np.array([])\n",
    "    intercepts = np.array([])\n",
    "    std_errs = np.array([])\n",
    "\n",
    "    slopes_pct = np.array([])\n",
    "    intercepts_pct = np.array([])\n",
    "    std_errs_pct = np.array([])\n",
    "\n",
    "    diffs_q1q4 = np.array([])\n",
    "    std_errs_q1q4 = np.array([])\n",
    "\n",
    "    diffs_d1d10 = np.array([])\n",
    "    std_errs_d1d10 = np.array([])\n",
    "\n",
    "    years = np.unique(dates_years)\n",
    "    yearsmask = np.full_like(years, True).astype('bool')\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, year)]\n",
    "\n",
    "        prediction = predictions[method].loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        trmratio = trmratios.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        mask = ~np.isnan(trmratio) & ~np.isnan(prediction)\n",
    "\n",
    "        if np.count_nonzero(mask) == 0 :\n",
    "\n",
    "            yearsmask[years==year] = False\n",
    "\n",
    "            continue\n",
    "\n",
    "            slopes = np.append(slopes, np.nan)\n",
    "            intercepts = np.append(intercepts, np.nan)\n",
    "            std_errs = np.append(std_errs, np.nan)\n",
    "\n",
    "            slopes_pct = np.append(slopes_pct, np.nan)\n",
    "            intercepts_pct = np.append(intercepts, np.nan)\n",
    "            std_errs_pct = np.append(std_errs_pct, np.nan)\n",
    "\n",
    "            diffs_q1q4 = np.append(diffs_q1q4, np.nan)\n",
    "            std_errs_q1q4 = np.append(std_errs_q1q4, np.nan)\n",
    "\n",
    "            diffs_d1d10 = np.append(diffs_d1d10, np.nan)\n",
    "            std_errs_d1d10 = np.append(std_errs_d1d10, np.nan)\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = sp.stats.linregress(trmratio[mask], prediction[mask])\n",
    "\n",
    "        slopes = np.append(slopes, slope)\n",
    "        intercepts = np.append(intercepts, intercept)\n",
    "        std_errs = np.append(std_errs, std_err)\n",
    "\n",
    "\n",
    "        slope_pct, intercept_pct, r_value_pct, p_value_pct, std_err_pct = sp.stats.linregress(intopercentile(trmratio[mask]), intopercentile(prediction[mask]))\n",
    "\n",
    "        slopes_pct = np.append(slopes_pct, slope_pct)\n",
    "        intercepts_pct = np.append(intercepts, intercept_pct)\n",
    "        std_errs_pct = np.append(std_errs_pct, std_err_pct)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'mean', bins=4)\n",
    "        stds, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'std', bins=4)\n",
    "        counts, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'count', bins=4)\n",
    "\n",
    "        diff_q1q4 = means[-1] - means[0]\n",
    "        std_err_q1q4 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_q1q4 = np.append(diffs_q1q4, diff_q1q4)\n",
    "        std_errs_q1q4 = np.append(std_errs_q1q4, std_err_q1q4)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'mean', bins=3)\n",
    "        stds, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'std', bins=3)\n",
    "        counts, _, _ = sp.stats.binned_statistic(intopercentile(prediction[mask]), trmratio[mask], 'count', bins=3)\n",
    "\n",
    "        diff_d1d10 = means[-1] - means[0]\n",
    "        std_err_d1d10 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_d1d10 = np.append(diffs_d1d10, diff_d1d10)\n",
    "        std_errs_d1d10 = np.append(std_errs_d1d10, std_err_d1d10)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.66,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    ax[0].errorbar(years[yearsmask], slopes, yerr=1.96*std_errs, fmt='C0o')\n",
    "    ax[0].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[0].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[0].set_title(\"Linear Regression: Trading Ratio vs Predicted Return\")\n",
    "\n",
    "    ax[1].errorbar(years[yearsmask], slopes_pct, yerr=1.96*std_errs_pct, fmt='C2o')\n",
    "    ax[1].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[1].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[1].set_title(\"Percentile Regression: Trading Ratio vs Predicted Return\")\n",
    "\n",
    "    ax[2].errorbar(years[yearsmask], diffs_q1q4, yerr=1.96*std_errs_q1q4, fmt='C4o')\n",
    "    ax[2].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[2].set_ylim([-0.045, 0.075])\n",
    "    ax[2].set_ylabel(\"4th Quantile Mean - 1st Quantile Mean\")\n",
    "    ax[2].set_title(\"Difference of Mean Trading Ratio \\n between 4th and 1st Predicted Return Quantile\", pad=-20)\n",
    "\n",
    "    ax[3].errorbar(years[yearsmask], diffs_d1d10, yerr=1.96*std_errs_d1d10, fmt='C6o')\n",
    "    ax[3].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[3].set_ylim([-0.045, 0.075])\n",
    "    ax[3].set_ylabel(\"10th Decile Mean - 1st Decile Mean\")\n",
    "    ax[3].set_title(\"Difference of Mean Trading Ratio \\n between 10th and 1st Predicted Return Decile\", pad=-10)\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(\"Trading Activity and Machine Learning Predictions - Time Series Analysis - \" + methodtitlelabel, size=18, y=1.02)\n",
    "    plt.savefig(graphpath+'trmratiovsprediction_'+method+'_tsanalysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "trmratiovsprediction_tsanalysis('forest', \"Random Forest\")\n",
    "trmratiovsprediction_tsanalysis('nn3', \"3-Layer Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for one intuitive year: 2015\n",
    "\n",
    "chosendates = dates[np.isin(dates_years, 2015)]\n",
    "print(chosendates)\n",
    "\n",
    "method = 'nn3'\n",
    "\n",
    "prediction = predictions['nn3'].loc[chosendates].to_numpy().flatten()\n",
    "prediction_pctile = predictions_pctile['nn3'].loc[chosendates].to_numpy().flatten()\n",
    "print(\"prediction.shape\", prediction.shape)\n",
    "print(\"predictions non-nans\", np.count_nonzero(~np.isnan(prediction)))\n",
    "\n",
    "shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "shortratio_pctile = shortratios_pctile.loc[chosendates].to_numpy().flatten()\n",
    "print(\"shortratio.shape\", shortratio.shape)\n",
    "print(\"shortratio non-nans\",  np.count_nonzero(~np.isnan(shortratio)))\n",
    "\n",
    "mask = ~np.isnan(shortratio) & ~np.isnan(prediction)\n",
    "\n",
    "print(\"points:\",  np.count_nonzero(mask))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12*1.33,12/2))\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "ax[0].plot(prediction[mask], shortratio[mask], 'ko', alpha=0.03)\n",
    "ax[0].set_xlabel('Monthly Predicted Return')\n",
    "ax[0].set_ylabel('Monthly Short Ratio')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlim([-0.5, 0.5])\n",
    "\n",
    "ax[1].plot(prediction_pctile[mask], shortratio_pctile[mask], 'ko', alpha=0.01)\n",
    "ax[1].set_xlabel('Monthly Predicted Return, Percentile')\n",
    "ax[1].set_ylabel('Monthly Short Ratio, Percentile')\n",
    "\n",
    "plt.savefig(graphpath+'shortratiovsprediction_nn3_2015.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortratiovsprediction_1995to2015(method, methodtitlelabel):\n",
    "\n",
    "    def shortvsreturnpercentilewithbinsplot(chosenyears, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, chosenyears)]\n",
    "\n",
    "        prediction = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "        shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        nanmask = ~np.isnan(shortratio) & ~np.isnan(prediction)\n",
    "\n",
    "         # We only draw 50.000 randomly chosen points on each graph\n",
    "        nanrandommask = np.full(len(shortratio[nanmask]), False)\n",
    "        nanrandommask[:5*10**4] = True\n",
    "        np.random.shuffle(nanrandommask)\n",
    "        randommask = nanmask.copy()\n",
    "        randommask[nanmask] = nanrandommask    \n",
    "\n",
    "        binedges = np.linspace(np.min(prediction[nanmask]), np.max(prediction[nanmask]), 21)\n",
    "        binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction[nanmask], shortratio[nanmask], 'mean', bins=binedges)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction[nanmask], shortratio[nanmask], 'std', bins=binedges)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction[nanmask], shortratio[nanmask], 'count', bins=binedges)\n",
    "\n",
    "        errs = 1.96 * stds / np.sqrt(counts)\n",
    "\n",
    "        ax.plot(prediction[randommask], shortratio[randommask], 'ko', alpha=alphaparam)\n",
    "        ax.plot(prediction[randommask], np.full_like(prediction[randommask], np.nanmean(shortratio)), 'k-', label='Overall average')\n",
    "        ax.errorbar(binpts, means, yerr=errs, fmt='C0o', label='Vigintile avg. (95% CI)')\n",
    "        ax.set_xlabel('Monthly Predicted Return, Percentile')\n",
    "        ax.set_ylabel('Monthly Short Ratio')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim([0.001, 1])\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.5,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([1995, 1996, 1997, 1998, 1999], ax[0], alphaparam=0.002, title=\"1995-1999\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2000, 2001, 2002, 2003, 2004], ax[1], alphaparam=0.002, title=\"2000-2004\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2005, 2006, 2007, 2008, 2009], ax[2], alphaparam=0.002, title=\"2005-2009\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2010, 2011, 2012, 2013, 2014, 2015], ax[3], alphaparam=0.002, title=\"2010-2015\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "\n",
    "    plt.savefig(graphpath+'shortratiovsprediction_'+method+'_1995to2015.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "shortratiovsprediction_1995to2015('nn3', \"3-Layer Neural Network\")\n",
    "shortratiovsprediction_1995to2015('forest', \"Random Forest\")\n",
    "\n",
    "shortratiovsprediction_1995to2015('ols', \"Ordinary Least Squares\")\n",
    "shortratiovsprediction_1995to2015('tree', \"Regression Tree\")\n",
    "shortratiovsprediction_1995to2015('nn1', \"1-Layer Neural Network\")\n",
    "shortratiovsprediction_1995to2015('nn2', \"2-Layer Neural Network\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortratiovsprediction_tsanalysis(method, methodtitlelabel):\n",
    "\n",
    "    slopes = np.array([])\n",
    "    intercepts = np.array([])\n",
    "    std_errs = np.array([])\n",
    "\n",
    "    slopes_pct = np.array([])\n",
    "    intercepts_pct = np.array([])\n",
    "    std_errs_pct = np.array([])\n",
    "\n",
    "    diffs_q1q4 = np.array([])\n",
    "    std_errs_q1q4 = np.array([])\n",
    "\n",
    "    diffs_d1d10 = np.array([])\n",
    "    std_errs_d1d10 = np.array([])\n",
    "\n",
    "    years = np.unique(dates_years)\n",
    "    yearsmask = np.full_like(years, True).astype('bool')\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, year)]\n",
    "\n",
    "        prediction = predictions[method].loc[chosendates].to_numpy().flatten()\n",
    "        prediction_pctile = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "        shortratio_pctile = shortratios_pctile.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        mask = ~np.isnan(shortratio) & ~np.isnan(prediction)\n",
    "\n",
    "        if np.count_nonzero(mask) == 0 :\n",
    "\n",
    "            yearsmask[years==year] = False\n",
    "\n",
    "            continue\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = sp.stats.linregress(shortratio[mask], prediction[mask])\n",
    "\n",
    "        slopes = np.append(slopes, slope)\n",
    "        intercepts = np.append(intercepts, intercept)\n",
    "        std_errs = np.append(std_errs, std_err)\n",
    "\n",
    "\n",
    "        slope_pct, intercept_pct, r_value_pct, p_value_pct, std_err_pct = sp.stats.linregress(shortratio_pctile[mask], prediction_pctile[mask])\n",
    "\n",
    "        slopes_pct = np.append(slopes_pct, slope_pct)\n",
    "        intercepts_pct = np.append(intercepts, intercept_pct)\n",
    "        std_errs_pct = np.append(std_errs_pct, std_err_pct)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'mean', bins=4)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'std', bins=4)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'count', bins=4)\n",
    "\n",
    "        diff_q1q4 = means[-1] - means[0]\n",
    "        std_err_q1q4 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_q1q4 = np.append(diffs_q1q4, diff_q1q4)\n",
    "        std_errs_q1q4 = np.append(std_errs_q1q4, std_err_q1q4)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'mean', bins=10)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'std', bins=10)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction_pctile[mask], shortratio[mask], 'count', bins=10)\n",
    "\n",
    "        diff_d1d10 = means[-1] - means[0]\n",
    "        std_err_d1d10 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_d1d10 = np.append(diffs_d1d10, diff_d1d10)\n",
    "        std_errs_d1d10 = np.append(std_errs_d1d10, std_err_d1d10)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.5,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    ax[0].errorbar(years[yearsmask], slopes, yerr=1.96*std_errs, fmt='C0o')\n",
    "    ax[0].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[0].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[0].set_title(\"Linear Regression: Short Ratio vs Predicted Return\")\n",
    "\n",
    "    ax[1].errorbar(years[yearsmask], slopes_pct, yerr=1.96*std_errs_pct, fmt='C2o')\n",
    "    ax[1].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[1].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[1].set_title(\"Percentile Regression: Short Ratio vs Predicted Return\")\n",
    "\n",
    "    ax[2].errorbar(years[yearsmask], diffs_q1q4, yerr=1.96*std_errs_q1q4, fmt='C4o')\n",
    "    ax[2].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[3].set_ylim([-0.045, 0.075])\n",
    "    ax[2].set_ylabel(\"4th Quantile Mean - 1st Quantile Mean\")\n",
    "    ax[2].set_title(\"Difference of Mean Short Ratio \\n between 4th and 1st Predicted Return Quantile\", pad=-20)\n",
    "\n",
    "    ax[3].errorbar(years[yearsmask], diffs_d1d10, yerr=1.96*std_errs_d1d10, fmt='C6o')\n",
    "    ax[3].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[3].set_ylim([-0.045, 0.075])\n",
    "    ax[3].set_ylabel(\"10th Decile Mean - 1st Decile Mean\")\n",
    "    ax[3].set_title(\"Difference of Mean Short Ratio \\n between 10th and 1st Predicted Return Decile\", pad=-10)\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "    plt.savefig(graphpath+'shortratiovsprediction_'+method+'_tsanalysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "shortratiovsprediction_tsanalysis('forest', \"Random Forest\")\n",
    "shortratiovsprediction_tsanalysis('nn3', \"3-Layer Neural Network\")\n",
    "\n",
    "shortratiovsprediction_tsanalysis('ols', \"Ordinary Least Squares\")\n",
    "shortratiovsprediction_tsanalysis('tree', \"Regression Tree\")\n",
    "shortratiovsprediction_tsanalysis('nn1', \"1-Layer Neural Network\")\n",
    "shortratiovsprediction_tsanalysis('nn2', \"2-Layer Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days to Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daystocovervsprediction_1995to2015(method, methodtitlelabel):\n",
    "\n",
    "    def daystocovervsreturnpercentilewithbinsplot(chosenyears, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, chosenyears)]\n",
    "\n",
    "        prediction = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "        daystocover = daystocovers.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        nanmask = ~np.isnan(daystocover) & ~np.isnan(prediction)\n",
    "\n",
    "         # We only draw 50.000 randomly chosen points on each graph\n",
    "        nanrandommask = np.full(len(daystocover[nanmask]), False)\n",
    "        nanrandommask[:5*10**4] = True\n",
    "        np.random.shuffle(nanrandommask)\n",
    "        randommask = nanmask.copy()\n",
    "        randommask[nanmask] = nanrandommask    \n",
    "\n",
    "        binedges = np.linspace(np.min(prediction[nanmask]), np.max(prediction[nanmask]), 21)\n",
    "        binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction[nanmask], daystocover[nanmask], 'mean', bins=binedges)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction[nanmask], daystocover[nanmask], 'std', bins=binedges)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction[nanmask], daystocover[nanmask], 'count', bins=binedges)\n",
    "\n",
    "        errs = 1.96 * stds / np.sqrt(counts)\n",
    "\n",
    "        ax.plot(prediction[randommask], daystocover[randommask], 'ko', alpha=alphaparam)\n",
    "        ax.plot(prediction[randommask], np.full_like(prediction[randommask], np.nanmean(daystocover)), 'k-', label='Overall average')\n",
    "        ax.errorbar(binpts, means, yerr=errs, fmt='C0o', label='Vigintile avg. (95% CI)')\n",
    "        ax.set_xlabel('Monthly Predicted Return, Percentile')\n",
    "        ax.set_ylabel('Monthly Short Ratio')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim([0.05, 5])\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.5,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "    daystocovervsreturnpercentilewithbinsplot([1995, 1996, 1997, 1998, 1999], ax[0], alphaparam=0.002, title=\"1995-1999\")\n",
    "\n",
    "    daystocovervsreturnpercentilewithbinsplot([2000, 2001, 2002, 2003, 2004], ax[1], alphaparam=0.002, title=\"2000-2004\")\n",
    "\n",
    "    daystocovervsreturnpercentilewithbinsplot([2005, 2006, 2007, 2008, 2009], ax[2], alphaparam=0.002, title=\"2005-2009\")\n",
    "\n",
    "    daystocovervsreturnpercentilewithbinsplot([2010, 2011, 2012, 2013, 2014, 2015], ax[3], alphaparam=0.002, title=\"2010-2015\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "\n",
    "    plt.savefig(graphpath+'daystocovervsprediction_'+method+'_1995to2015.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "daystocovervsprediction_1995to2015('nn3', \"3-Layer Neural Network\")\n",
    "daystocovervsprediction_1995to2015('forest', \"Random Forest\")\n",
    "daystocovervsprediction_1995to2015('ols', \"Ordinary Least Squares\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daystocovervsprediction_tsanalysis(method, methodtitlelabel):\n",
    "\n",
    "    slopes = np.array([])\n",
    "    intercepts = np.array([])\n",
    "    std_errs = np.array([])\n",
    "\n",
    "    slopes_pct = np.array([])\n",
    "    intercepts_pct = np.array([])\n",
    "    std_errs_pct = np.array([])\n",
    "\n",
    "    diffs_q1q4 = np.array([])\n",
    "    std_errs_q1q4 = np.array([])\n",
    "\n",
    "    diffs_d1d10 = np.array([])\n",
    "    std_errs_d1d10 = np.array([])\n",
    "\n",
    "    years = np.unique(dates_years)\n",
    "    yearsmask = np.full_like(years, True).astype('bool')\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, year)]\n",
    "\n",
    "        prediction = predictions[method].loc[chosendates].to_numpy().flatten()\n",
    "        prediction_pctile = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        daystocover = daystocovers.loc[chosendates].to_numpy().flatten()\n",
    "        daystocover_pctile = daystocovers_pctile.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        mask = ~np.isnan(daystocover) & ~np.isnan(prediction)\n",
    "\n",
    "        if np.count_nonzero(mask) == 0 :\n",
    "\n",
    "            yearsmask[years==year] = False\n",
    "\n",
    "            continue\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = sp.stats.linregress(daystocover[mask], prediction[mask])\n",
    "\n",
    "        slopes = np.append(slopes, slope)\n",
    "        intercepts = np.append(intercepts, intercept)\n",
    "        std_errs = np.append(std_errs, std_err)\n",
    "\n",
    "\n",
    "        slope_pct, intercept_pct, r_value_pct, p_value_pct, std_err_pct = sp.stats.linregress(daystocover_pctile[mask], prediction_pctile[mask])\n",
    "\n",
    "        slopes_pct = np.append(slopes_pct, slope_pct)\n",
    "        intercepts_pct = np.append(intercepts, intercept_pct)\n",
    "        std_errs_pct = np.append(std_errs_pct, std_err_pct)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'mean', bins=4)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'std', bins=4)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'count', bins=4)\n",
    "\n",
    "        diff_q1q4 = means[-1] - means[0]\n",
    "        std_err_q1q4 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_q1q4 = np.append(diffs_q1q4, diff_q1q4)\n",
    "        std_errs_q1q4 = np.append(std_errs_q1q4, std_err_q1q4)\n",
    "\n",
    "\n",
    "        means, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'mean', bins=10)\n",
    "        stds, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'std', bins=10)\n",
    "        counts, _, _ = sp.stats.binned_statistic(prediction[mask], daystocover[mask], 'count', bins=10)\n",
    "\n",
    "        diff_d1d10 = means[-1] - means[0]\n",
    "        std_err_d1d10 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "        diffs_d1d10 = np.append(diffs_d1d10, diff_d1d10)\n",
    "        std_errs_d1d10 = np.append(std_errs_d1d10, std_err_d1d10)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.5,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    ax[0].errorbar(years[yearsmask], slopes, yerr=1.96*std_errs, fmt='C0o')\n",
    "    ax[0].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[0].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[0].set_title(\"Linear Regression: Days to Cover vs Predicted Return\")\n",
    "\n",
    "    ax[1].errorbar(years[yearsmask], slopes_pct, yerr=1.96*std_errs_pct, fmt='C2o')\n",
    "    ax[1].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[1].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[1].set_title(\"Percentile Regression: Days to Cover vs Predicted Return\")\n",
    "\n",
    "    ax[2].errorbar(years[yearsmask], diffs_q1q4, yerr=1.96*std_errs_q1q4, fmt='C4o')\n",
    "    ax[2].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[2].set_ylim([-1, 1])\n",
    "    ax[2].set_ylabel(\"4th Quantile Mean - 1st Quantile Mean\")\n",
    "    ax[2].set_title(\"Difference of Mean Days to Cover \\n between 4th and 1st Predicted Return Quantile\", pad=-20)\n",
    "\n",
    "    ax[3].errorbar(years[yearsmask], diffs_d1d10, yerr=1.96*std_errs_d1d10, fmt='C6o')\n",
    "    ax[3].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[3].set_ylim([-1, 1])\n",
    "    ax[3].set_ylabel(\"10th Decile Mean - 1st Decile Mean\")\n",
    "    ax[3].set_title(\"Difference of Mean Days to Cover \\n between 10th and 1st Predicted Return Decile\", pad=-10)\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "    plt.savefig(graphpath+'daystocovervsprediction_'+method+'_tsanalysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "daystocovervsprediction_tsanalysis('forest', \"Random Forest\")\n",
    "daystocovervsprediction_tsanalysis('nn3', \"3-Layer Neural Network\")\n",
    "daystocovervsprediction_tsanalysis('ols', \"Ordinary Least Squares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Interest by Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mktcaplims_p95 = np.nanpercentile(mktcaps.values, 95, axis=1)\n",
    "mktcaplims_p75 = np.nanpercentile(mktcaps.values, 75, axis=1)\n",
    "\n",
    "\n",
    "mask_bigcap = (mktcaps_pctile.ge(95))\n",
    "mask_midcap = (mktcaps_pctile.lt(95) & mktcaps_pctile.ge(75))\n",
    "mask_smallcap = (mktcaps_pctile.lt(75))\n",
    "\n",
    "\n",
    "plt.plot(mktcaplims_p95)\n",
    "plt.plot(mktcaplims_p75)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(mktcaplims_p95[-1])\n",
    "print(mktcaplims_p75[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortratiovsprediction_1995to2015(method, methodtitlelabel):\n",
    "\n",
    "    def shortvsreturnpercentilewithbinsplot(chosenyears, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, chosenyears)]\n",
    "\n",
    "        prediction = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "        mktcappctile = mktcaps_pctile.loc[chosendates].to_numpy().flatten()\n",
    "        shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        # Mask for nans and marketcaps:\n",
    "        nanmask = ~np.isnan(shortratio) & ~np.isnan(prediction)\n",
    "        \n",
    "        bigcapmask = (mktcappctile>=95)\n",
    "        midcapmask = (95>mktcappctile) & (mktcappctile>=50)\n",
    "        smallcapmask = (50>mktcappctile)\n",
    "        \n",
    "        # We only draw 50.000 randomly chosen points on each graph\n",
    "        nanrandommask = np.full(len(shortratio[nanmask]), False)\n",
    "        nanrandommask[:5*10**4] = True\n",
    "        np.random.shuffle(nanrandommask)\n",
    "        randommask = nanmask.copy()\n",
    "        randommask[nanmask] = nanrandommask    \n",
    "\n",
    "        binedges = np.linspace(np.min(prediction[nanmask]), np.max(prediction[nanmask]), 21)\n",
    "        binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "        means_big, _, _ = sp.stats.binned_statistic(prediction[nanmask & bigcapmask], shortratio[nanmask & bigcapmask], 'mean', bins=binedges)\n",
    "        stds_big, _, _ = sp.stats.binned_statistic(prediction[nanmask & bigcapmask], shortratio[nanmask & bigcapmask], 'std', bins=binedges)\n",
    "        counts_big, _, _ = sp.stats.binned_statistic(prediction[nanmask & bigcapmask], shortratio[nanmask & bigcapmask], 'count', bins=binedges)\n",
    "\n",
    "        errs_big = 1.96 * stds_big / np.sqrt(counts_big)\n",
    "        \n",
    "        \n",
    "        means_mid, _, _ = sp.stats.binned_statistic(prediction[nanmask & midcapmask], shortratio[nanmask & midcapmask], 'mean', bins=binedges)\n",
    "        stds_mid, _, _ = sp.stats.binned_statistic(prediction[nanmask & midcapmask], shortratio[nanmask & midcapmask], 'std', bins=binedges)\n",
    "        counts_mid, _, _ = sp.stats.binned_statistic(prediction[nanmask & midcapmask], shortratio[nanmask & midcapmask], 'count', bins=binedges)\n",
    "\n",
    "        errs_mid = 1.96 * stds_mid / np.sqrt(counts_mid)\n",
    "        \n",
    "        \n",
    "        means_small, _, _ = sp.stats.binned_statistic(prediction[nanmask & smallcapmask], shortratio[nanmask & smallcapmask], 'mean', bins=binedges)\n",
    "        stds_small, _, _ = sp.stats.binned_statistic(prediction[nanmask & smallcapmask], shortratio[nanmask & smallcapmask], 'std', bins=binedges)\n",
    "        counts_small, _, _ = sp.stats.binned_statistic(prediction[nanmask & smallcapmask], shortratio[nanmask & smallcapmask], 'count', bins=binedges)\n",
    "\n",
    "        errs_small = 1.96 * stds_small / np.sqrt(counts_small)\n",
    "\n",
    "        ax.plot(prediction[randommask], shortratio[randommask], 'ko', alpha=alphaparam)\n",
    "        ax.plot(prediction[randommask], np.full_like(prediction[randommask], np.nanmean(shortratio)), 'k-', label='Overall average')\n",
    "        ax.errorbar(binpts, means_small, yerr=errs_small, fmt='C6o', label='Small cap - Vigintile avg. (95% CI)')\n",
    "        ax.errorbar(binpts, means_mid, yerr=errs_mid, fmt='C2o', label='Mid cap - Vigintile avg. (95% CI)')\n",
    "        ax.errorbar(binpts, means_big, yerr=errs_big, fmt='C4o', label='Big cap - Vigintile avg. (95% CI)')\n",
    "        ax.set_xlabel('Monthly Predicted Return, Percentile')\n",
    "        ax.set_ylabel('Monthly Short Ratio')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim([0.005, 5])\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10*1.5,10))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([1995, 1996, 1997, 1998, 1999], ax[0], alphaparam=0.002, title=\"1995-1999\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2000, 2001, 2002, 2003, 2004], ax[1], alphaparam=0.002, title=\"2000-2004\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2005, 2006, 2007, 2008, 2009], ax[2], alphaparam=0.002, title=\"2005-2009\")\n",
    "\n",
    "    shortvsreturnpercentilewithbinsplot([2010, 2011, 2012, 2013, 2014, 2015], ax[3], alphaparam=0.002, title=\"2010-2015\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "\n",
    "    plt.savefig(graphpath+'shortratiovsprediction_'+method+'_1995to2015_bycap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "shortratiovsprediction_1995to2015('nn3', \"3-Layer Neural Network\")\n",
    "shortratiovsprediction_1995to2015('forest', \"Random Forest\")\n",
    "\n",
    "shortratiovsprediction_1995to2015('ols', \"Ordinary Least Squares\")\n",
    "shortratiovsprediction_1995to2015('tree', \"Regression Tree\")\n",
    "shortratiovsprediction_1995to2015('nn1', \"1-Layer Neural Network\")\n",
    "shortratiovsprediction_1995to2015('nn2', \"2-Layer Neural Network\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortratiovsprediction_tsanalysis(method, methodtitlelabel):\n",
    "\n",
    "    slopes_big = np.array([])\n",
    "    std_errs_big = np.array([])\n",
    "\n",
    "    slopes_mid = np.array([])\n",
    "    std_errs_mid = np.array([])\n",
    "    \n",
    "    slopes_small = np.array([])\n",
    "    std_errs_small = np.array([])\n",
    "\n",
    "    years = np.unique(dates_years)\n",
    "    yearsmask = np.full_like(years, True).astype('bool')\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        chosendates = dates[np.isin(dates_years, year)]\n",
    "\n",
    "        prediction = predictions_pctile[method].loc[chosendates].to_numpy().flatten()\n",
    "        mktcappctile = mktcaps_pctile.loc[chosendates].to_numpy().flatten()\n",
    "        shortratio = shortratios_pctile.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "        nanmask = ~np.isnan(shortratio) & ~np.isnan(prediction)\n",
    "                \n",
    "        bigcapmask = (mktcappctile>=95)\n",
    "        midcapmask = (95>mktcappctile) & (mktcappctile>=50)\n",
    "        smallcapmask = (50>mktcappctile)\n",
    "\n",
    "        if np.count_nonzero(nanmask & bigcapmask) * np.count_nonzero(nanmask & midcapmask) * np.count_nonzero(nanmask & smallcapmask)== 0 :\n",
    "\n",
    "            yearsmask[years==year] = False\n",
    "\n",
    "            continue\n",
    "\n",
    "        slope_big, _, _, _, std_err_big = sp.stats.linregress(shortratio[nanmask & bigcapmask], prediction[nanmask & bigcapmask])\n",
    "\n",
    "        slopes_big = np.append(slopes_big, slope_big)\n",
    "        std_errs_big = np.append(std_errs_big, std_err_big)\n",
    "        \n",
    "        \n",
    "        slope_mid, _, _, _, std_err_mid = sp.stats.linregress(shortratio[nanmask & midcapmask], prediction[nanmask & midcapmask])\n",
    "\n",
    "        slopes_mid = np.append(slopes_mid, slope_mid)\n",
    "        std_errs_mid = np.append(std_errs_mid, std_err_mid)\n",
    "\n",
    "    \n",
    "        slope_small, _, _, _, std_err_small = sp.stats.linregress(shortratio[nanmask & smallcapmask], prediction[nanmask & smallcapmask])\n",
    "\n",
    "        slopes_small = np.append(slopes_small, slope_small)\n",
    "        std_errs_small = np.append(std_errs_small, std_err_small)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(6*3.5,6), sharey=True)\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    ax[0].errorbar(years[yearsmask], slopes_big, yerr=1.96*std_errs_big, fmt='C0o')\n",
    "    ax[0].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[0].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[0].set_title(\"Big Cap\")\n",
    "\n",
    "    ax[1].errorbar(years[yearsmask], slopes_mid, yerr=1.96*std_errs_mid, fmt='C2o')\n",
    "    ax[1].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[1].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[1].set_title(\"Mid Cap\")\n",
    "\n",
    "    ax[2].errorbar(years[yearsmask], slopes_small, yerr=1.96*std_errs_small, fmt='C4o')\n",
    "    ax[2].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "    ax[2].set_ylabel(\"Regression Coefficient\")\n",
    "    ax[2].set_title(\"Small Cap\")\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    fig.suptitle(methodtitlelabel, size=20, y=1.02)\n",
    "    plt.savefig(graphpath+'shortratiovsprediction_'+method+'_tsanalysis_bycap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "shortratiovsprediction_tsanalysis('forest', \"Random Forest\")\n",
    "shortratiovsprediction_tsanalysis('nn3', \"3-Layer Neural Network\")\n",
    "\n",
    "shortratiovsprediction_tsanalysis('ols', \"Ordinary Least Squares\")\n",
    "shortratiovsprediction_tsanalysis('tree', \"Regression Tree\")\n",
    "shortratiovsprediction_tsanalysis('nn1', \"1-Layer Neural Network\")\n",
    "shortratiovsprediction_tsanalysis('nn2', \"2-Layer Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-publication decline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs with publication dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubdates = np.array(['1805', '1986', '1943', '2005', '1901', '1966', '1959',\n",
    "                     '1995', '2002', '1986', '1986', '1986', '1986', '1986'], dtype='datetime64')\n",
    "\n",
    "methodpubdates = dict(zip(methods, pubdates))\n",
    "print(methodpubdates)\n",
    "\n",
    "pubs = ['Legendre (1805)', 'Santosa and Symes (1986)', 'Tikhonov (1943)', 'Zou and Hastie (2005)',\n",
    "              'Pearson (1901)', 'Wold (1966)', 'Belson (1959)', 'Ho (1995)', 'Friedman (2002)',\n",
    "              'McClelland et al. (1986)', 'McClelland et al. (1986)', 'McClelland et al. (1986)',\n",
    "              'McClelland et al. (1986)', 'McClelland et al. (1986)']\n",
    "\n",
    "methodpubs = dict(zip(methods, pubs))\n",
    "print(methodpubs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'enet'\n",
    "\n",
    "i = 0\n",
    "labels = ['Enet']\n",
    "\n",
    "def pubdategraph(method, label, hlaboffsets=[0, 0, 0]):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.spines['right'].set_visible(False)\n",
    "        axis.spines['top'].set_visible(False)\n",
    "\n",
    "    oos_spearman = scores.loc[(dates,method,'oos','spearman')].fillna(method='ffill').to_numpy()\n",
    "\n",
    "    ax[0].plot(dtdates, oos_spearman, 'C5', alpha=0.5, label='Spearman Rho')\n",
    "    ax[0].plot(dtdates, ma(oos_spearman, window_size=12*5, centered=True), 'C4', label='5-year avg.')\n",
    "    ax[0].axvline(x=methodpubdates[method], color='k', linestyle='--')\n",
    "    ax[0].text(methodpubdates[method] + hlaboffsets[0], 0.97, methodpubs[method], bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    ax[0].set_ylim([-0.05, 1.05])\n",
    "    ax[0].legend(frameon=False, loc='upper left')\n",
    "    ax[0].set_title(label + \" : OOS Spearman Rho\", pad=10)\n",
    "\n",
    "\n",
    "    ax[1].plot(dtdates, yearrets_l10[method], 'C1', label='Returns')\n",
    "    ax[1].plot(dtdates, ma(yearrets_l10[method], window_size=12*3, centered=True), 'C0', label='3-year avg.')\n",
    "    ax[1].axvline(x=methodpubdates[method], color='k', linestyle='--')\n",
    "    ax[1].text(methodpubdates[method] + hlaboffsets[1], 0.27, methodpubs[method], bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    ax[1].set_ylim([-0.1, 0.3])\n",
    "    ax[1].legend(frameon=False, loc='upper left')\n",
    "    ax[1].set_title(label + \" : Monthly returns\", pad=10)\n",
    "\n",
    "    ax[2].plot(dtdates, ff3alphas_3y_rank[method], 'C2')\n",
    "    ax[2].axvline(x=methodpubdates[method], color='k', linestyle='--')\n",
    "    ax[2].text(methodpubdates[method] + hlaboffsets[2], 0.135, methodpubs[method], bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    ax[2].set_ylim([-0.05, 0.15])\n",
    "    ax[2].set_title(label + \" : 3-year FF3 alpha\", pad=10)\n",
    "    \n",
    "    if np.isin(method, ['nn1', 'nn2', 'nn3', 'nn4', 'nn5']):\n",
    "        \n",
    "        # Add Werbos (1974)\n",
    "        ax[0].axvline(x=np.datetime64('1974'), ymax=0.8, color='k', linestyle='--')\n",
    "        ax[0].text(np.datetime64('1974'), 0.02, 'Werbos (1974)', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "        \n",
    "        ax[1].axvline(x=np.datetime64('1974'), ymax=0.8, color='k', linestyle='--')\n",
    "        ax[1].text(np.datetime64('1974'), -0.08, 'Werbos (1974)', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "        \n",
    "        ax[2].axvline(x=np.datetime64('1974'), color='k', linestyle='--')\n",
    "        ax[2].text(np.datetime64('1974'), -0.04, 'Werbos (1974)', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "\n",
    "\n",
    "    plt.savefig(graphpath+'pubdategraph_' + method + '.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "hlaboffsets = [np.timedelta64(14,'Y'), np.timedelta64(14,'Y'), np.timedelta64(0,'Y')]\n",
    "pubdategraph('lasso', 'Lasso', hlaboffsets)\n",
    "\n",
    "pubdategraph('enet', 'Enet')\n",
    "\n",
    "pubdategraph('forest', 'Forest')\n",
    "\n",
    "pubdategraph('gbrt', 'GBRT')\n",
    "\n",
    "hlaboffsets = [np.timedelta64(12,'Y'), np.timedelta64(12,'Y'), np.timedelta64(0,'Y')]\n",
    "pubdategraph('nn1', 'NN1', hlaboffsets)\n",
    "pubdategraph('nn2', 'NN2', hlaboffsets)\n",
    "pubdategraph('nn3', 'NN3', hlaboffsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication-date aligned graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmethods = ['lasso', 'enet', 'forest', 'gbrt', 'nn1', 'nn2', 'nn2']\n",
    "labels = ['Lasso', 'Enet', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3']\n",
    "\n",
    "nyears = 25\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ylabels = np.linspace(-nyears, nyears, 2*12*nyears)\n",
    "        \n",
    "for (i,method) in enumerate(plotmethods):\n",
    "    \n",
    "    pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "    \n",
    "    oos_spearman = scores.loc[(dates,method,'oos','spearman')].fillna(method='ffill').to_numpy()\n",
    "    \n",
    "    plotspearman = oos_spearman[max(pubidx-12*nyears, 0):min(pubidx+12*nyears, T)]\n",
    "    \n",
    "    plotspearman = np.append(np.array([np.nan]*(min(pubidx-12*nyears, 0))), plotspearman)    \n",
    "    plotspearman = np.append(plotspearman, np.array([np.nan]*(max(pubidx+12*nyears-T, 0))))\n",
    "    \n",
    "    plotspearman_ma = ma(plotspearman, window_size=12*3, centered=True)\n",
    "        \n",
    "    ax.plot(ylabels, plotspearman_ma, label=labels[i] + ' (' + str(methodpubdates[method]) + ')')\n",
    "\n",
    "ax.plot(ylabels, np.zeros_like(ylabels), 'k', linewidth=0.75)\n",
    "\n",
    "ax.axvline(x=0, color='k', linestyle='--')\n",
    "ax.text(x=0, y=1, s='Publication Date', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "\n",
    "ax.set_xlabel('Years before and after publication')\n",
    "ax.set_ylabel('Out-of-sample Spearman rho, \\n 3-year centered moving average')\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "\n",
    "ax.legend(frameon=False, loc='upper left')\n",
    "plt.savefig(graphpath+'pubdategraph_aligned_spearman.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmethods = ['lasso', 'enet', 'forest', 'gbrt', 'nn1', 'nn2', 'nn2']\n",
    "labels = ['Lasso', 'Enet', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3']\n",
    "\n",
    "nyears = 25\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ylabels = np.linspace(-nyears, nyears, 2*12*nyears)\n",
    "        \n",
    "for (i,method) in enumerate(plotmethods):\n",
    "    \n",
    "    pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "    \n",
    "    plotrets = yearrets_l10[method][max(pubidx-12*nyears, 0):min(pubidx+12*nyears, T)]\n",
    "    \n",
    "    plotrets = np.append(np.array([np.nan]*(min(pubidx-12*nyears, 0))), plotrets)    \n",
    "    plotrets = np.append(plotrets, np.array([np.nan]*(max(pubidx+12*nyears-T, 0))))\n",
    "    \n",
    "    plotrets_ma = ma(plotrets, window_size=12*3, centered=True)\n",
    "        \n",
    "    ax.plot(ylabels, plotrets_ma, label=labels[i] + ' (' + str(methodpubdates[method]) + ')')\n",
    "\n",
    "ax.plot(ylabels, np.zeros_like(ylabels), 'k', linewidth=0.75)\n",
    "\n",
    "ax.axvline(x=0, color='k', linestyle='--')\n",
    "ax.text(x=0, y=0.15, s='Publication Date', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "\n",
    "ax.set_xlabel('Years before and after publication')\n",
    "ax.set_ylabel('3-year centered moving average return')\n",
    "ax.set_ylim([-0.02, 0.16])\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "plt.savefig(graphpath+'pubdategraph_aligned_returns.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmethods = ['lasso', 'enet', 'forest', 'gbrt', 'nn1', 'nn2', 'nn2']\n",
    "labels = ['Lasso', 'Enet', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3']\n",
    "\n",
    "nyears = 25\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ylabels = np.linspace(-nyears, nyears, 2*12*nyears)\n",
    "        \n",
    "for (i,method) in enumerate(plotmethods):\n",
    "    \n",
    "    pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "    \n",
    "    plotalphas = ff3alphas_3y_rank[method][max(pubidx-12*nyears, 0):min(pubidx+12*nyears, T)]\n",
    "    \n",
    "    plotalphas = np.append(np.array([np.nan]*(min(pubidx-12*nyears, 0))), plotalphas)    \n",
    "    plotalphas = np.append(plotalphas, np.array([np.nan]*(max(pubidx+12*nyears-T, 0))))\n",
    "            \n",
    "    ax.plot(ylabels, plotalphas, label=labels[i] + ' (' + str(methodpubdates[method]) + ')')\n",
    "\n",
    "ax.plot(ylabels, np.zeros_like(ylabels), 'k', linewidth=0.75)\n",
    "\n",
    "ax.axvline(x=0, color='k', linestyle='--')\n",
    "ax.text(x=0, y=0.105, s='Publication Date', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "\n",
    "ax.set_xlabel('Years before and after publication')\n",
    "ax.set_ylabel('3-year Fama-French 3-factor alpha')\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "plt.savefig(graphpath+'pubdategraph_aligned_alphas.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmethods = ['lasso', 'enet', 'forest', 'gbrt', 'nn1', 'nn2', 'nn2']\n",
    "labels = ['Lasso', 'Enet', 'Forest', 'GBRT', 'NN1', 'NN2', 'NN3']\n",
    "\n",
    "nyears = 25\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "        \n",
    "for (i,method) in enumerate(['nn3']):\n",
    "    \n",
    "    pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "\n",
    "    nmonths_before = min(12*nyears, pubidx)    \n",
    "    nmonths_after = min(12*nyears, T - pubidx)\n",
    "\n",
    "    rets = yearrets_l10[method][pubidx-nmonths_before:pubidx+nmonths_after]\n",
    "\n",
    "    dummy = np.zeros_like(rets)\n",
    "    dummy[12*nyears:] = 1\n",
    "\n",
    "    X0 = np.ones_like(rets)\n",
    "    X1 = np.arange(len(rets)) - pubidx\n",
    "\n",
    "    exog = np.array([dummy, X0, X1, X1**2, X1**3, X1**4, X1**5]).T\n",
    "    exog = pd.DataFrame(exog, columns=['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5'])\n",
    "\n",
    "    model1 = sm.OLS(rets,exog[['dummy', 'X0', 'X1',]]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "    model2 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "    model5 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "\n",
    "    ylabels = np.linspace(-nmonths_before/12, nmonths_after/12, (nmonths_before+nmonths_after))\n",
    "            \n",
    "    ax.plot(ylabels, rets, label='Returns')\n",
    "    ax.text(x=0, y=0.6, s='McClelland et al. (1986)', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "    ax.plot(ylabels, model1.predict(exog[['dummy', 'X0', 'X1',]]), 'C2', label='Linear fit')\n",
    "    ax.plot(ylabels, model2.predict(exog[['dummy', 'X0', 'X1', 'X2']]), 'C4', label='Quadratic fit')\n",
    "    ax.plot(ylabels, model5.predict(exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]), 'C6', label='Quintic fit')\n",
    "\n",
    "ax.plot(ylabels, np.zeros_like(ylabels), 'k', linewidth=0.75)\n",
    "\n",
    "ax.axvline(x=0, color='k', linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Years before and after publication')\n",
    "ax.set_ylabel('Monthly returns')\n",
    "\n",
    "ax.set_title('3-Layer Neural Network - Returns')\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "plt.savefig(graphpath+'fitgraph_aligned_returns.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_table = ['lasso', 'enet']\n",
    "methods_table2 = ['forest', 'gbrt']\n",
    "methods_table3 = ['nn1', 'nn2', 'nn2']\n",
    "\n",
    "maxnyears = 15\n",
    "\n",
    "def returnmodels(methodslist):\n",
    "    \n",
    "    modellist = []\n",
    "        \n",
    "    for (i,method) in enumerate(methodslist):\n",
    "\n",
    "        pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "\n",
    "        nmonths_before = min(12*nyears, pubidx)    \n",
    "        nmonths_after = min(12*nyears, T - pubidx)\n",
    "\n",
    "        rets = 100 * yearrets_l10[method][pubidx-nmonths_before:pubidx+nmonths_after]\n",
    "\n",
    "        dummy = np.zeros_like(rets)\n",
    "        dummy[12*nyears:] = 1\n",
    "\n",
    "        X0 = np.ones_like(rets)\n",
    "        X1 = np.arange(len(rets)) - pubidx\n",
    "\n",
    "        exog = np.array([dummy, X0, X1, X1**2, X1**3, X1**4, X1**5]).T\n",
    "        exog = pd.DataFrame(exog, columns=['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5'])\n",
    "\n",
    "        model1 = sm.OLS(rets,exog[['dummy', 'X0', 'X1',]]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model2 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model5 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "\n",
    "        modellist.append(model1)\n",
    "        modellist.append(model2)\n",
    "        modellist.append(model5)\n",
    "            \n",
    "    return modellist\n",
    "\n",
    "\n",
    "pystout(models=returnmodels(methods_table),\n",
    "        file=tablespath+'postpubdecline_returns_ls10_table.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Lasso - Returns':[1,3],'Enet - Returns':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table2),\n",
    "        file=tablespath+'postpubdecline_returns_ls10_table2.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Forest - Returns':[1,3],'GBRT - Returns':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table3),\n",
    "        file=tablespath+'postpubdecline_returns_ls10_table3.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*3,\n",
    "        varlabels={'dummy':'Post-Pub. Dummy'},\n",
    "        mgroups={'NN1 - Returns':[1,3],'NN2 - Returns':[4,6],'NN3 - Returns':[7,9]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_table = ['lasso', 'enet']\n",
    "methods_table2 = ['forest', 'gbrt']\n",
    "methods_table3 = ['nn1', 'nn2', 'nn2']\n",
    "\n",
    "maxnyears = 15\n",
    "\n",
    "def returnmodels(methodslist):\n",
    "    \n",
    "    modellist = []\n",
    "        \n",
    "    for (i,method) in enumerate(methodslist):\n",
    "\n",
    "        pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "\n",
    "        nmonths_before = min(12*nyears, pubidx)    \n",
    "        nmonths_after = min(12*nyears, T - pubidx)\n",
    "\n",
    "        rets = 100 * yearrets_rank[method][pubidx-nmonths_before:pubidx+nmonths_after]\n",
    "\n",
    "        dummy = np.zeros_like(rets)\n",
    "        dummy[12*nyears:] = 1\n",
    "\n",
    "        X0 = np.ones_like(rets)\n",
    "        X1 = np.arange(len(rets))\n",
    "\n",
    "        exog = np.array([dummy, X0, X1, X1**2, X1**3, X1**4, X1**5]).T\n",
    "        exog = pd.DataFrame(exog, columns=['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5'])\n",
    "\n",
    "        model1 = sm.OLS(rets,exog[['dummy', 'X0', 'X1',]]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model2 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model5 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "\n",
    "        modellist.append(model1)\n",
    "        modellist.append(model2)\n",
    "        modellist.append(model5)\n",
    "            \n",
    "    return modellist\n",
    "\n",
    "\n",
    "pystout(models=returnmodels(methods_table),\n",
    "        file=tablespath+'postpubdecline_returns_rank_table.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Lasso - Returns':[1,3],'Enet - Returns':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table2),\n",
    "        file=tablespath+'postpubdecline_returns_rank_table2.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Forest - Returns':[1,3],'GBRT - Returns':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table3),\n",
    "        file=tablespath+'postpubdecline_returns_rank_table3.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*3,\n",
    "        varlabels={'dummy':'Post-Pub. Dummy'},\n",
    "        mgroups={'NN1 - Returns':[1,3],'NN2 - Returns':[4,6],'NN3 - Returns':[7,9]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_table = ['lasso', 'enet']\n",
    "methods_table2 = ['forest', 'gbrt']\n",
    "methods_table3 = ['nn1', 'nn2', 'nn2']\n",
    "\n",
    "maxnyears = 15\n",
    "\n",
    "def returnmodels(methodslist):\n",
    "    \n",
    "    modellist = []\n",
    "        \n",
    "    for (i,method) in enumerate(methodslist):\n",
    "\n",
    "        pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "\n",
    "        nmonths_before = min(12*nyears, pubidx)    \n",
    "        nmonths_after = min(12*nyears, T - pubidx)\n",
    "\n",
    "        rets = 100 * yearrets_l10[method][pubidx-nmonths_before:pubidx+nmonths_after]\n",
    "\n",
    "        dummy = np.zeros_like(rets)\n",
    "        dummy[12*nyears:] = 1\n",
    "\n",
    "        X0 = np.ones_like(rets)\n",
    "        X1 = np.arange(len(rets))\n",
    "\n",
    "        exog = np.array([dummy, X0, X1, X1**2, X1**3, X1**4, X1**5]).T\n",
    "        exog = pd.DataFrame(exog, columns=['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5'])\n",
    "\n",
    "        model1 = sm.OLS(rets,exog[['dummy', 'X0', 'X1',]]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model2 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model5 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "\n",
    "        modellist.append(model1)\n",
    "        modellist.append(model2)\n",
    "        modellist.append(model5)\n",
    "            \n",
    "    return modellist\n",
    "\n",
    "\n",
    "pystout(models=returnmodels(methods_table),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_ls10_table.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Lasso - 3Y-FF3-Alpha':[1,3],'Enet - 3Y-FF3-Alpha':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table2),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_ls10_table2.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Forest - 3Y-FF3-Alpha':[1,3],'GBRT - 3Y-FF3-Alpha':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table3),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_ls10_table3.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*3,\n",
    "        varlabels={'dummy':'Post-Pub. Dummy'},\n",
    "        mgroups={'NN1 - 3Y-FF3-Alpha':[1,3],'NN2 - 3Y-FF3-Alpha':[4,6],'NN3 - 3Y-FF3-Alpha':[7,9]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_table = ['lasso', 'enet']\n",
    "methods_table2 = ['forest', 'gbrt']\n",
    "methods_table3 = ['nn1', 'nn2', 'nn2']\n",
    "\n",
    "maxnyears = 15\n",
    "\n",
    "def returnmodels(methodslist):\n",
    "    \n",
    "    modellist = []\n",
    "        \n",
    "    for (i,method) in enumerate(methodslist):\n",
    "\n",
    "        pubidx = np.where(dtdates>=methodpubdates[method])[0][0]\n",
    "\n",
    "        nmonths_before = min(12*nyears, pubidx)    \n",
    "        nmonths_after = min(12*nyears, T - pubidx)\n",
    "\n",
    "        rets = 100 * ff3alphas_3y_rank[method][pubidx-nmonths_before:pubidx+nmonths_after]\n",
    "\n",
    "        dummy = np.zeros_like(rets)\n",
    "        dummy[12*nyears:] = 1\n",
    "\n",
    "        X0 = np.ones_like(rets)\n",
    "        X1 = np.arange(len(rets))\n",
    "\n",
    "        exog = np.array([dummy, X0, X1, X1**2, X1**3, X1**4, X1**5]).T\n",
    "        exog = pd.DataFrame(exog, columns=['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5'])\n",
    "\n",
    "        model1 = sm.OLS(rets,exog[['dummy', 'X0', 'X1',]]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model2 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "        model5 = sm.OLS(rets,exog[['dummy', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5']]).fit(cov_type='HAC',cov_kwds={'maxlags':12*10})\n",
    "\n",
    "        modellist.append(model1)\n",
    "        modellist.append(model2)\n",
    "        modellist.append(model5)\n",
    "            \n",
    "    return modellist\n",
    "\n",
    "\n",
    "pystout(models=returnmodels(methods_table),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_rank_table.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Lasso - 3Y-FF3-Alpha':[1,3],'Enet - 3Y-FF3-Alpha':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table2),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_rank_table2.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*2,\n",
    "        varlabels={'dummy':'Post-Publication Dummy'},\n",
    "        mgroups={'Forest - 3Y-FF3-Alpha':[1,3],'GBRT - 3Y-FF3-Alpha':[4,6]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})\n",
    "\n",
    "pystout(models=returnmodels(methods_table3),\n",
    "        file=tablespath+'postpubdecline_ff3alphas_rank_table3.tex',\n",
    "        exogvars = ['dummy'],\n",
    "        digits=2,\n",
    "        endog_names=['Linear','Quadratic','Quintic']*3,\n",
    "        varlabels={'dummy':'Post-Pub. Dummy'},\n",
    "        mgroups={'NN1 - 3Y-FF3-Alpha':[1,3],'NN2 - 3Y-FF3-Alpha':[4,6],'NN3 - 3Y-FF3-Alpha':[7,9]},\n",
    "        stars={.1:'*',.05:'**',.01:'***'},\n",
    "        modstat={'nobs':'Obs','rsquared_adj':'Adj. R\\sym{2}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Prescience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shortvsreturnpercentilewithbinsplot(chosenyears, ax, alphaparam=0.01, title=\"\"):\n",
    "\n",
    "    chosendates = dates[np.isin(dates_years, chosenyears)]\n",
    "\n",
    "    rets = returns_pctile.loc[chosendates].to_numpy().flatten()\n",
    "    shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "    nanmask = ~np.isnan(shortratio) & ~np.isnan(rets)\n",
    "\n",
    "     # We only draw 100.000 randomly chosen points on each graph\n",
    "    nanrandommask = np.full(len(shortratio[nanmask]), False)\n",
    "    nanrandommask[:5*10**4] = True\n",
    "    np.random.shuffle(nanrandommask)\n",
    "    randommask = nanmask.copy()\n",
    "    randommask[nanmask] = nanrandommask    \n",
    "\n",
    "    binedges = np.linspace(np.min(rets[nanmask]), np.max(rets[nanmask]), 21)\n",
    "    binpts = (binedges[1:] + binedges[:-1]) / 2\n",
    "\n",
    "    means, _, _ = sp.stats.binned_statistic(rets[nanmask], shortratio[nanmask], 'mean', bins=binedges)\n",
    "    stds, _, _ = sp.stats.binned_statistic(rets[nanmask], shortratio[nanmask], 'std', bins=binedges)\n",
    "    counts, _, _ = sp.stats.binned_statistic(rets[nanmask], shortratio[nanmask], 'count', bins=binedges)\n",
    "\n",
    "    errs = 1.96 * stds / np.sqrt(counts)\n",
    "\n",
    "    ax.plot(rets[randommask], shortratio[randommask], 'ko', alpha=alphaparam)\n",
    "    ax.plot(rets[randommask], np.full_like(rets[randommask], np.nanmean(shortratio)), 'k-', label='Overall average')\n",
    "    ax.errorbar(binpts, means, yerr=errs, fmt='C0o', label='Vigintile avg. (95% CI)')\n",
    "    ax.set_xlabel('Monthly Return, Percentile')\n",
    "    ax.set_ylabel('Monthly Short Ratio')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([0.005, 1])\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12*1.33,12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "shortvsreturnpercentilewithbinsplot([1995, 1996, 1997, 1998, 1999], ax[0], alphaparam=0.002, title=\"1995-1999\")\n",
    "\n",
    "shortvsreturnpercentilewithbinsplot([2000, 2001, 2002, 2003, 2004], ax[1], alphaparam=0.002, title=\"2000-2004\")\n",
    "\n",
    "shortvsreturnpercentilewithbinsplot([2005, 2006, 2007, 2008, 2009], ax[2], alphaparam=0.002, title=\"2005-2009\")\n",
    "\n",
    "shortvsreturnpercentilewithbinsplot([2010, 2011, 2012, 2013, 2014, 2015], ax[3], alphaparam=0.002, title=\"2010-2015\")\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.suptitle(\"Short Interest and Actual Returns - Cross-Section Analysis\", size=18, y=1.02)\n",
    "\n",
    "plt.savefig(graphpath+'shortratiovsreturns_1995to2015.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = np.array([])\n",
    "intercepts = np.array([])\n",
    "std_errs = np.array([])\n",
    "\n",
    "slopes_pct = np.array([])\n",
    "intercepts_pct = np.array([])\n",
    "std_errs_pct = np.array([])\n",
    "\n",
    "diffs_q1q4 = np.array([])\n",
    "std_errs_q1q4 = np.array([])\n",
    "\n",
    "diffs_d1d10 = np.array([])\n",
    "std_errs_d1d10 = np.array([])\n",
    "\n",
    "years = np.unique(dates_years)\n",
    "yearsmask = np.full_like(years, True).astype('bool')\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    chosendates = dates[np.isin(dates_years, year)]\n",
    "\n",
    "    rets = returns.loc[chosendates].to_numpy().flatten()\n",
    "    rets_pctile = returns.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "    shortratio = shortratios.loc[chosendates].to_numpy().flatten()\n",
    "    shortratio_pctile = shortratios_pctile.loc[chosendates].to_numpy().flatten()\n",
    "\n",
    "    mask = ~np.isnan(shortratio) & ~np.isnan(rets)\n",
    "\n",
    "    if np.count_nonzero(mask) == 0 :\n",
    "\n",
    "        yearsmask[years==year] = False\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(shortratio[mask], rets[mask])\n",
    "\n",
    "    slopes = np.append(slopes, slope)\n",
    "    intercepts = np.append(intercepts, intercept)\n",
    "    std_errs = np.append(std_errs, std_err)\n",
    "\n",
    "\n",
    "    slope_pct, intercept_pct, r_value_pct, p_value_pct, std_err_pct = sp.stats.linregress(shortratio_pctile[mask], rets_pctile[mask])\n",
    "\n",
    "    slopes_pct = np.append(slopes_pct, slope_pct)\n",
    "    intercepts_pct = np.append(intercepts, intercept_pct)\n",
    "    std_errs_pct = np.append(std_errs_pct, std_err_pct)\n",
    "\n",
    "\n",
    "    means, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'mean', bins=10)\n",
    "    stds, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'std', bins=10)\n",
    "    counts, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'count', bins=10)\n",
    "\n",
    "    diff_q1q4 = means[-1] - means[0]\n",
    "    std_err_q1q4 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "    diffs_q1q4 = np.append(diffs_q1q4, diff_q1q4)\n",
    "    std_errs_q1q4 = np.append(std_errs_q1q4, std_err_q1q4)\n",
    "\n",
    "\n",
    "    means, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'mean', bins=10)\n",
    "    stds, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'std', bins=10)\n",
    "    counts, _, _ = sp.stats.binned_statistic(rets_pctile[mask], shortratio[mask], 'count', bins=10)\n",
    "\n",
    "    diff_d1d10 = means[-1] - means[0]\n",
    "    std_err_d1d10 = np.sqrt(((stds[-1]**2)/counts[-1])+((stds[0]**2)/counts[0]))\n",
    "\n",
    "    diffs_d1d10 = np.append(diffs_d1d10, diff_d1d10)\n",
    "    std_errs_d1d10 = np.append(std_errs_d1d10, std_err_d1d10)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10*1.66,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for axis in ax:\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "ax[0].errorbar(years[yearsmask], slopes, yerr=1.96*std_errs, fmt='C0o')\n",
    "ax[0].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "ax[0].set_ylabel(\"Regression Coefficient\")\n",
    "ax[0].set_title(\"Linear Regression: Short Ratio vs Return\")\n",
    "ax[0].set_ylim([-0.06, 0.06])\n",
    "    \n",
    "ax[1].errorbar(years[yearsmask], slopes_pct, yerr=1.96*std_errs_pct, fmt='C2o')\n",
    "ax[1].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "ax[1].set_ylabel(\"Regression Coefficient\")\n",
    "ax[1].set_title(\"Percentile Regression: Short Ratio vs Return\")\n",
    "ax[1].set_ylim([-0.0003, 0.0003])\n",
    "\n",
    "ax[2].errorbar(years[yearsmask], diffs_q1q4, yerr=1.96*std_errs_q1q4, fmt='C4o')\n",
    "ax[2].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "ax[2].set_ylim([-0.1, 0.1])\n",
    "ax[2].set_ylabel(\"4th Quantile Mean - 1st Quantile Mean\")\n",
    "ax[2].set_title(\"Difference of Mean Short Ratio \\n between 4th and 1st Return Quantile\", pad=-20)\n",
    "\n",
    "ax[3].errorbar(years[yearsmask], diffs_d1d10, yerr=1.96*std_errs_d1d10, fmt='C6o')\n",
    "ax[3].plot(years[yearsmask], np.zeros_like(years[yearsmask]), 'k')\n",
    "ax[3].set_ylim([-0.1, 0.1])\n",
    "ax[3].set_ylabel(\"10th Decile Mean - 1st Decile Mean\")\n",
    "ax[3].set_title(\"Difference of Mean Short Ratio \\n between 10th and 1st Return Decile\", pad=-10)\n",
    "\n",
    "def addpubdates(ax, y):\n",
    "\n",
    "    ax.axvline(x=1974, ymax=1, color='k', linestyle='--')\n",
    "    ax.text(1976, y, 'Neural Networks', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "    ax.axvline(x=1986, ymax=1, color='k', linestyle='--')\n",
    "    ax.text(1986, y, 'Lasso', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "    ax.axvline(x=1995, ymax=1, color='k', linestyle='--')\n",
    "    ax.text(1995, y, 'Forest', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "    ax.axvline(x=2002, ymax=1, color='k', linestyle='--')\n",
    "    ax.text(2002, y, 'GBRT', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "    ax.axvline(x=2005, ymax=1, color='k', linestyle='--')\n",
    "    ax.text(2006, y, 'Enet', bbox=dict(boxstyle=\"round\", fc=\"1\"), ha='center')\n",
    "    \n",
    "addpubdates(ax[0], -0.055)\n",
    "addpubdates(ax[1], -0.00027)\n",
    "addpubdates(ax[2], -0.09)\n",
    "addpubdates(ax[3], -0.09)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.suptitle(\"Market Prescience - Time Series Analysis\", size=18, y=1.02)\n",
    "plt.savefig(graphpath+'shortratiovsreturns_tsanalysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assocmethods = ['ols', 'tree', 'nn3']\n",
    "assoclabels = ['Ordinary Least Squares', 'Regression Tree', '3-Layer Neural Network']\n",
    "\n",
    "\n",
    "pearson_rets = np.array([])\n",
    "spearman_rets = np.array([])\n",
    "\n",
    "\n",
    "pearson_prediction = dict()\n",
    "spearman_prediction = dict()\n",
    "\n",
    "for method in assocmethods :\n",
    "\n",
    "    pearson_prediction[method] = np.array([])\n",
    "    spearman_prediction[method] = np.array([])\n",
    "\n",
    "\n",
    "datesmask = np.full_like(dates, True).astype('bool')\n",
    "datesmask[dates<=19800000] = False\n",
    "\n",
    "for date in dates[datesmask]:\n",
    "        \n",
    "    rets = returns.loc[date].to_numpy().flatten()\n",
    "    \n",
    "    shortratio = shortratios_i.loc[date].to_numpy().flatten()\n",
    "\n",
    "    mask = ~np.isnan(shortratio) & ~np.isnan(rets)\n",
    "    \n",
    "    pearson_rets = np.append(pearson_rets, sp.stats.pearsonr(shortratio[mask], rets[mask])[0])\n",
    "    spearman_rets = np.append(spearman_rets, sp.stats.spearmanr(shortratio[mask], rets[mask])[0])\n",
    "\n",
    "    for method in assocmethods :\n",
    "        \n",
    "        prediction = predictions[method].loc[date].to_numpy().flatten()\n",
    "    \n",
    "        pearson_prediction[method] = np.append(pearson_prediction[method], sp.stats.pearsonr(shortratio[mask], prediction[mask])[0])\n",
    "\n",
    "        spearman_prediction[method] = np.append(spearman_prediction[method], sp.stats.spearmanr(shortratio[mask], prediction[mask])[0])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8*1.66,8))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.plot(dtdates[datesmask], pearson_rets, 'k', alpha=0.2)\n",
    "ax.plot(dtdates[datesmask], ma(pearson_rets, window_size=12*3, centered=True), 'k', label=\"True Returns (3-year avg.)\")\n",
    "\n",
    "colours = ['C0', 'C2', 'C4', 'C6']\n",
    "\n",
    "for (i,method) in enumerate(assocmethods):\n",
    "    \n",
    "    ax.plot(dtdates[datesmask], pearson_prediction[method], colours[i], alpha=0.2)\n",
    "    ax.plot(dtdates[datesmask], ma(pearson_prediction[method], window_size=12*3, centered=True), colours[i], label=\"Pred. rets. : \" + assoclabels[i] + \" (3-year avg.)\")\n",
    "\n",
    "ax.plot(dtdates[datesmask], np.zeros_like(dates[datesmask]), 'k--', linewidth=1.5)\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\"Relationship between True or Predicted Returns and Monthly Short Interest\", size=18)\n",
    "ax.set_ylabel(\"Pearson Correlation Coefficient\")\n",
    "plt.savefig(graphpath+'marketprescience_spearman.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8*1.66,8))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.plot(dtdates[datesmask], pearson_rets, 'k', alpha=0.2)\n",
    "ax.plot(dtdates[datesmask], ma(pearson_rets, window_size=12*3, centered=True), 'k', label=\"True Returns (3-year avg.)\")\n",
    "\n",
    "colours = ['C0', 'C2', 'C4', 'C6']\n",
    "\n",
    "for (i,method) in enumerate(assocmethods):\n",
    "    \n",
    "    ax.plot(dtdates[datesmask], spearman_prediction[method], colours[i], alpha=0.2)\n",
    "    ax.plot(dtdates[datesmask], ma(spearman_prediction[method], window_size=12*3, centered=True), colours[i], label=\"Pred. rets. : \" + assoclabels[i] + \" (3-year avg.)\")\n",
    "\n",
    "ax.plot(dtdates[datesmask], np.zeros_like(dates[datesmask]), 'k--', linewidth=1.5)\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\"Relationship between True or Predicted Returns and Monthly Short Interest\", size=18)\n",
    "ax.set_ylabel(\"Spearman Rho\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'ols'\n",
    "print(method)\n",
    "\n",
    "date = dates[-100]\n",
    "print(date)\n",
    "\n",
    "rets = returns.loc[date].to_numpy().flatten()\n",
    "\n",
    "shortratio = shortratios_i.loc[date].to_numpy().flatten()\n",
    "\n",
    "prediction = predictions[method].loc[date].to_numpy().flatten()\n",
    "\n",
    "mask = ~np.isnan(shortratio) & ~np.isnan(rets)\n",
    "\n",
    "print(sp.stats.pearsonr(rets[mask], shortratio[mask]))\n",
    "print(sp.stats.pearsonr(prediction[mask], shortratio[mask]))\n",
    "print(sp.stats.pearsonr(rets[mask], prediction[mask]))\n",
    "\n",
    "print(\"sp.stats.pearsonr(rets[mask], rets[mask] - prediction[mask])\")\n",
    "print(sp.stats.pearsonr(shortratio[mask], rets[mask] - prediction[mask]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(6*4.2,6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "ax[0].plot(rets, shortratio, 'o', alpha=0.05)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlim([-0.75, 0.75])\n",
    "ax[0].set_xlabel('rets')\n",
    "ax[0].set_ylabel('shortratio')\n",
    "\n",
    "ax[1].plot(prediction, shortratio, 'o', alpha=0.05)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlim([-0.75, 0.75])\n",
    "ax[1].set_xlabel('prediction')\n",
    "ax[1].set_ylabel('shortratio')\n",
    "\n",
    "prederror = rets - prediction\n",
    "\n",
    "ax[2].plot(prederror, shortratio, 'o', alpha=0.05)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlim([-0.75, 0.75])\n",
    "ax[2].set_xlabel('prederror')\n",
    "ax[2].set_ylabel('shortratio')\n",
    "\n",
    "ax[3].plot(rets, prediction, 'o', alpha=0.05)\n",
    "ax[3].set_xlim([-0.5, 0.5])\n",
    "ax[3].set_xlabel('rets')\n",
    "ax[3].set_ylabel('prediction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Cf3-cBgh78on"
   ],
   "machine_shape": "hm",
   "name": "portfolioevaluation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
